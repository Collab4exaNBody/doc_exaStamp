{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Index","text":"exascale Simulation of       Time-dependent Atomic and       Molecular systems in Parallel.      <ul> <li> <p> Features</p> <ul> <li>Classical Molecular Dynamics</li> <li>Polymers, Molecules, Metals, Ceramics</li> <li>Semi-empirical and ML Potentials</li> <li>High strain rate, Mechanics, Shock loading</li> <li>Massively parallel MPI x OpenMP x GPU</li> </ul> </li> <li> <p> Access to the code</p> <ul> <li> GitHub repository to access the code and fork the repository for new developments.</li> <li> Open a new issue to report bugs.</li> <li> Create a pull request to ask for your developments to be merged.</li> </ul> </li> <li> <p> Latest Releases</p> <ul> <li> V3.7.2</li> </ul> </li> <li> <p> Open Source, Apache 2.0</p> <p>exaStamp is the result of a long-time and collaborative effort at CEA, France. It is an open-source code, distributed under the terms of the Apache Public License version 2.0.</p> <p> License</p> </li> <li> <p> Team</p> <ul> <li> <p>Thierry Carrard, CEA DAM DIF - </p> </li> <li> <p>Paul Lafourcade, CEA DAM DIF -   </p> </li> <li> <p>Rapha\u00ebl Prat, CEA DES -   </p> </li> </ul> </li> <li> <p> Contact</p> <p>Use the  Github discussions to suggest new features, discuss ideas, check annoucements or show what you're doing with the code.</p> <p>See the About page for contact and license information.</p> </li> </ul>"},{"location":"About/index.html","title":"About exaStamp","text":""},{"location":"About/index.html#project-summary","title":"Project summary","text":"<p>exaStamp is a high performance molecular dynamics simulation code, originated at CEA/DAM. exaStamp stands for Simulations Temporelles Atomistiques et Mol\u00e9culaires Parall\u00e8les \u00e0 l'exascale (in French) or exascale Simulations of Time-dependent Atomistic and Molecular systems in Parallel (in English). This software is distributed under the Apache 2.0 public license.  </p> <p>exaStamp dependes on the exaNBody framework<sup>1</sup> and is designed to run efficiently on supercomputers as well as laptops or workstations. It takes advantage of hybrid parallelism with the ability to run using MPI + X where X is either OpenMP or Cuda/HIP. This code is the result of a long-time effort at CEA/DAM/DIF, France. It is an open-source code, distributed freely under the terms of the Apache Public License version 2.0.</p>"},{"location":"About/index.html#contributors","title":"Contributors","text":"<ul> <li> <p> Development team</p> <ul> <li> <p>  Thierry Carrard</p> </li> <li> <p>  Paul Lafourcade</p> </li> <li> <p>  Rapha\u00ebl Prat</p> </li> </ul> </li> <li> <p> Scientific team</p> <ul> <li>  A</li> <li>  B</li> </ul> </li> <li> <p> Students</p> <ul> <li>  A</li> <li>  B</li> </ul> </li> </ul>"},{"location":"About/index.html#license","title":"License","text":"<p>exaStamp is licensed under the  Apache License, Version 2.0 (Apache-2.0).</p> <p>See LICENSE for details.</p>"},{"location":"About/index.html#contact","title":"Contact","text":"<p>Use the  Github discussions to suggest new features, discuss ideas, check annoucements or show what you're doing with the code.</p> <p>In addition, if you identify a malfuncton or a bug, please feel free to open an issue or discussion on Github.</p> <ol> <li> <p>Thierry Carrard, Rapha\u00ebl Prat, Guillaume Latu, Killian Babilotte, Paul Lafourcade, Lhassan Amarsid, and Laurent Soulard. Exanbody: a hpc framework for n-body applications. In Euro-Par 2023: Parallel Processing Workshops, 342\u2013354. Cham, 2024. Springer Nature Switzerland.\u00a0\u21a9</p> </li> </ol>"},{"location":"Background/index.html","title":"exaStamp background","text":"<p>This section provides users with a little background about the exaStamp molecular dynamics code as well as with information about the code distributions, releases notes and main features.. </p> <ul> <li>Overview Rapid overview of the exaStamp code.</li> <li>Distribution Practical information about the GitHub repository.</li> <li>Release Notes Release notes for each available release.</li> <li>Main Features General description of exaStamp main features.</li> </ul>"},{"location":"Background/Distribution/index.html","title":"Distribution","text":"<p>To access the exaStamp GitHub repository, please use the GitHub link below (left). If, by running the code or during the installation procedure, you were to identify a problem or if you would like to suggest any improvements or require a new feature, please use the \"Open a New Issue\" link below.</p> exaStamp GitHub Repository Open a New Issue"},{"location":"Background/Features/index.html","title":"Main Features","text":""},{"location":"Background/Features/index.html#general-features","title":"General features","text":"<ul> <li>Shared-memory multithreading (OpenMP)</li> <li>Message-passing parallelism (MPI)</li> <li>Graphics Processing Units (GPUs) via CUDA and HIP</li> <li>Can run on a single processor or with MPI only, though multithreading is recommended</li> <li>With MPI, spatial decomposition via Recursive Coordinate Bisection (RCB)</li> <li>Open-source under the Apache License 2.0</li> <li>Highly portable, modular C++20 code built on the Onika/exaNBody framework</li> <li>Entire simulation graph can be user-defined in the input file (simulation block)</li> <li>Easy to extend with new features and functionality</li> <li>Input files use an extended YAML-based format</li> <li>Run one or multiple simulations concurrently from a single script</li> <li>Assign independent resources to run the simulation and perform on-the-fly analysis</li> <li>Large suite of regression tests</li> </ul>"},{"location":"Background/Features/index.html#particle-types","title":"Particle types","text":"<ul> <li>Atoms (metals, oxides, ceramics)</li> <li>Rigid molecules</li> <li>Flexible molecules (polymers, molecular crystals)</li> </ul>"},{"location":"Background/Features/index.html#interatomic-potentials","title":"Interatomic potentials","text":"<ul> <li>Pair potentials: Buckingham, Exp-6, Lennard-Jones, ZBL, Morse, tabulated</li> <li>Short-range electrostatics: Coulombic</li> <li>Long-range electrostatics: Ewald, Wolf, reaction-field</li> <li>Many-body potentials: EAM, MEAM</li> <li>Machine-learning interatomic potentials (MLIPs): SNAP, N2P2, ACE</li> <li>Interface to the OpenKIM repository of potentials</li> <li>Bond potentials: harmonic, FENE, Morse, nonlinear, Class II (COMPASS), quartic (breakable), tabulated, scripted</li> <li>Angle potentials:</li> <li>Dihedral potentials:</li> <li>Improper potentials:</li> <li>Hybrid potentials: combine multiple pair, bond, angle, dihedral, and improper terms in a single simulation</li> <li>Overlay potentials: superposition of multiple pair and many-body potentials</li> </ul>"},{"location":"Background/Features/index.html#particles-creation","title":"Particles creation","text":"<ul> <li>Read external files (<code>.xyz</code>, <code>.data</code>, <code>.mpiio</code>)</li> <li>Create atoms on specific spatial regions </li> <li>Logical operation to define regions and populate them with particles</li> <li>Replication of the system</li> <li>Uniform or gaussian oise on atomic positions, atomic velocities and forces in specific regions</li> </ul>"},{"location":"Background/Features/index.html#thermodynamic-and-boundary-conditions","title":"Thermodynamic and boundary conditions","text":"<ul> <li>General triclinic simulation domain</li> <li>Isochoric-isoenergetic (NVE) ensemble</li> <li>Isochoric-isothermal (NVT) ensemble</li> <li>Isobaric-Isothermal (NPT) ensemble</li> <li>Two-temperature model (TTM)</li> <li>Parrinello/Rahman, Nos\u00e9-Hoover integrators</li> <li>Region-wise thermostatting of particles</li> <li>Pressure or component-wise stress barostats using Nos\u00e9/Hoover</li> <li>Deformation paths formalism for straining the box (any type of deformation can be applied)</li> <li>Periodic, Free, Mirror boundary conditions</li> </ul>"},{"location":"Background/Features/index.html#time-integration","title":"Time integration","text":"<ul> <li>Velocity-Verlet integrator</li> <li>User-defined time integrator</li> <li>Rigid body integration for rigid molecules</li> <li>Energy minimization via conjugate gradient and relaxation dynamics</li> </ul>"},{"location":"Background/Features/index.html#statistics-and-diagnostics","title":"Statistics and diagnostics","text":"<ul> <li>Simulation graph</li> <li>Performance results</li> <li>Highly customizable configuration</li> </ul>"},{"location":"Background/Features/index.html#on-the-fly-analysis","title":"On-the-fly analysis","text":"<ul> <li>Particle to grid projection</li> <li>Local average of per-particle field</li> <li>Local structural metrics (entropy, number of neighbors, centrosymmetry, crystal structure classification)</li> <li>Local mechanical metrics (deformation gradient tensor, velocity gradient tensor, slip/twinning system identification)</li> <li>Connected Component Labelling</li> <li>Slice projection and longitudinal profile calculation</li> <li>Histograms</li> <li>Radial distribution function (type-wise)</li> </ul>"},{"location":"Background/Features/index.html#output","title":"Output","text":"<ul> <li>Thermodynamic state and logging file</li> <li>Dump output on fixed and variable intervals, based on timestep or simulated time</li> <li>Simulation snapshots (<code>.xyz</code>, <code>.vtk</code>, <code>.vtp</code>) for OVITO and ParaView</li> <li>Binary restart files</li> <li>Per-atom quantities that can be appended to particles in simulation snapshots</li> </ul>"},{"location":"Background/Overview/index.html","title":"Overview","text":"<p>exaStamp is a software package for particle-based computational simulations. It stands for \u201cSimulations Temporelles Atomistiques et Mol\u00e9culaires Parall\u00e8le \u00e0 l\u2019Exascale\u201d (French) or \u201cexascale Simulations of Time-dependent Atomistic and Molecular systems in Parallel\u201d (English). exaStamp is a classical molecular dynamics (MD) code developed within the exaNBody<sup>1</sup> framework, allowing users to model systems of particles in any state (liquid, solid, or gas). For example, polymers, molecular crystals, and dense crystalline materials such as metals, ceramics, and oxides can be simulated with exaStamp. A wide range of force fields and boundary conditions is available, enabling complex simulations with up to a few billion particles.</p> <p>In general terms, exaStamp integrates Newton\u2019s equations of motion for collections of interacting particles. Each particle can represent an atom or a molecule. Most interaction models in exaStamp are short-ranged, with some long-range models also available. exaStamp uses neighbor cells to track nearby particles; these cell lists are optimized for systems with strong short-range repulsion, so local particle density remains manageable. On parallel machines, exaStamp employs the RCB (Recursive Coordinate Bisection) spatial-decomposition technique with <code>MPI</code> parallelization to partition the simulation domain into subdomains of roughly equal computational cost (i.e. similar number of particles), each assigned to a processor. Processes communicate and store \u201cghost\u201d data for atoms near subdomain boundaries. The code targets microscopic-scale modeling of material behavior under extreme conditions, with an emphasis on high strain rates, shock physics, and small-scale mechanics.</p> <p>exaStamp can be built and run on a single laptop or desktop, but it is primarily designed for parallel computers. It adopts a hybrid parallelization approach combining <code>MPI</code> (Message Passing Interface) and threads (<code>OpenMP</code>) to optimize computational cost and improve efficiency. Additionally, exaStamp supports <code>MPI</code> + GPUs via the <code>CUDA</code> programming model integrated through the <code>Onika</code> layer, enabling GPU acceleration for substantial performance gains. Written in <code>C++20</code>, exaStamp provides researchers and engineers with a modern tool for MD simulations in physics and mechanics.</p> <p>At first glance, exaStamp may seem difficult to modify or extend with new features\u2014such as additional interatomic potentials, thermodynamic conditions, or on-the-fly analysis tools. However, a developer\u2019s guide will soon be made available for those who wish to contribute.</p> <ol> <li> <p>Thierry Carrard, Rapha\u00ebl Prat, Guillaume Latu, Killian Babilotte, Paul Lafourcade, Lhassan Amarsid, and Laurent Soulard. Exanbody: a hpc framework for n-body applications. In Euro-Par 2023: Parallel Processing Workshops, 342\u2013354. Cham, 2024. Springer Nature Switzerland.\u00a0\u21a9</p> </li> </ol>"},{"location":"Background/Releases/index.html","title":"Versioning","text":""},{"location":"Background/Releases/index.html#code-releases","title":"Code releases","text":"<p>exaStamp releases can be directly accessed through the following Releases page as <code>.zip</code> or <code>.tar.gz</code> compressed files.</p> exaStamp Releases"},{"location":"Background/Releases/index.html#compatibility-with-onika-exanbody","title":"Compatibility with onika &amp; exaNBody","text":"<p>Please note that each version of exaStamp is compatible with a stable version of onika and exaNBody. Below are listed the correspondences. Any build or installation that do not respect these guidelines might lead to errors. If you want us to be able to provide the appropriate support, please follow the indications below.</p> exaStamp exaNBody onika main main main v3.7.0 v2.0.0 v1.0.0 v3.7.2 v2.0.5 v1.0.4"},{"location":"Background/Releases/index.html#release-notes","title":"Release notes","text":"<p>Release notes for the following releases of exaStamp can be directly accessed through the links below.</p> <ul> <li>Release 3.7.2 </li> <li>Release 3.7.3 </li> </ul>"},{"location":"Background/Releases/Release_3.7.2.html","title":"Release 3.7.2","text":""},{"location":"Background/Releases/Release_3.7.2.html#small-changes","title":"Small changes","text":"<ul> <li>Use the release mode as default build type mode</li> <li>add CONTRIBUTING.md + update guidelines in README.md </li> <li>add SUPPORT.md</li> <li>supressed paraview and xyz specific writers as they're obsolete. These operators are now defined in exaNBody directly</li> </ul>"},{"location":"Background/Releases/Release_3.7.2.html#eam-potentials","title":"EAM potentials","text":"<p>EAM potentials now use exaNBody's generalized attributes for the local electronic density and embedding functions used in EAM potentials. This now allows to visualize the local electronic density as a per-atom field in OVITO or ParaView for example.  </p> <p>Usage example (if an EAM potential is used!) :</p> <pre><code>write_xyz:\n  fields: [ rho_dEmb ]\n</code></pre> Image caption <p>:::{figure} /_static/eam_rho.gif :width: 40% :align: center :::</p>"},{"location":"Background/Releases/Release_3.7.2.html#spatial-average","title":"Spatial average","text":"<p>Thanks to new functionalities added in exaNBody@v2.0.5, a spatial averaging function is now available in exaStamp. </p> <p>Usage example: </p> <pre><code>average_neighbors_scalar:\n  nbh_field: mass\n  avg_field: avgmass\n  rcut: 8.0 ang\n  weight_function: [ 1.0, 0.0, 0.0, 0.0 ]\n\nwrite_xyz:\n  fields: [ mass, avgmass ]\n  filename: \"test_avg.xyz\"\n</code></pre> <p>Figure generated using ovito from test_avg.xyz file:</p> Image caption <p>Full Changelog: https://github.com/Collab4exaNBody/exaStamp/compare/release-v3.7.0...v3.7.2</p>"},{"location":"Background/Releases/Release_3.7.3.html","title":"Release 3.7.3","text":""},{"location":"Background/Releases/Release_3.7.3.html#small-changes","title":"Small changes","text":"<ul> <li>A</li> <li>B</li> <li>C</li> </ul> <p>Full Changelog: https://github.com/Collab4exaNBody/exaStamp/compare/release-v3.7.2...v3.7.3</p>"},{"location":"Beginner/index.html","title":"Beginner's guide to exaStamp","text":"<p>This section aims at providing a step-by-step tutorial on how to create and design a classical molecular dynamics simulation using <code>exaStamp</code>. The application takes advantage of YAML and can be fully defined using a combination of YAML nodes that act as individual operators. This allows a fine customization of the simulation by the user. Default configuration files are located in <code>exaStamp/data/config</code> and the basic ones are explained below.  </p>"},{"location":"Beginner/Basics/ExampleInputDeck.html","title":"A first, short example","text":""},{"location":"Beginner/Basics/ExampleInputDeck.html#individual-building-blocks","title":"Individual building blocks","text":""},{"location":"Beginner/Basics/ExampleInputDeck.html#global-block-definition","title":"<code>global</code> block definition","text":"<p>Below is an example of the <code>global</code> YAML block:</p> <pre><code>   global:\n     timestep: 0                      \n     physical_time: 0.                \n     dt: 1.0e-3 ps                    \n     rcut_inc: 1.0 ang                \n     log_mode: default                \n     simulation_log_frequency: 10     \n     simulation_end_iteration: 10000  \n     init_temperature: 300. K\n</code></pre> <p>allowing to define a simulation of 10000 steps with a 1 fs timestep where the thermodynamic state is printed on the screen every 10 steps. The screen log mode is set to the <code>default</code> mode for which the timestep, physical time, total energy, kinetic energy, potential energy, temperature, pressure and simulation status are printed. The initial temperature is also set to 300. K using a gaussian distribution of atomic velocities.</p>"},{"location":"Beginner/Basics/ExampleInputDeck.html#domain-block-definition","title":"<code>domain</code> block definition","text":"<p>The <code>domain</code> block allows the user to entirely define the simulation domain in which the particles will evolve dynamically. It contains multiple parameters that have to be consistent with each other:</p> <pre><code>   domain:\n     cell_size: 6.6 ang\n     grid_dims: [10, 10, 10]\n     bounds: [[ 0.0 ang,  0.0 ang,  0.0 ang],\n              [66.0 ang, 66.0 ang, 66.0 ang]]\n     periodic: [true,true,true]\n     expandable: false\n</code></pre> <p>Define as above, the simulation domain will be a cubic box with side length equal to 72.3 ang with a total of 1000 cells that define a 3D-periodic grid used for parallelism. In addition, the variable <code>expandable</code> set to false means that when creating particles, the domain should not be expanded to contain particles that are outside of it.</p>"},{"location":"Beginner/Basics/ExampleInputDeck.html#species-block-definition","title":"<code>species</code> block definition","text":"<p>The <code>species</code> block allows the definition of the particles' species that will be considered in the simulation.</p> <pre><code>   species:\n     - Ta:\n         mass: 180.95 Da\n         z: 73\n         charge: 0 e-\n</code></pre> <p>Here, the system will only be constituted of Tantalum particles.</p>"},{"location":"Beginner/Basics/ExampleInputDeck.html#compute_force-block-definition","title":"**<code>compute_force</code> block definition","text":"<p>Probably one of the most important YAML block when it comes to classical molecular dynamics, the <code>compute_force</code> block allows the user to define the interatomic potential that will be used and from which the forces acting on atoms will be derived.</p> <pre><code>   compute_force: johnson_force\n\n   johnson_force:\n     rcut: 6.1 ang\n     parameters:\n       re: 2.860082 ang\n       fe: 3.08634 eV/ang\n       rhoe: 33.787168 eV/ang\n       alpha: 8.489528\n       beta: 4.527748\n       A: 0.611679 eV\n       B: 1.032101 eV\n       kappa: 0.176977\n       lambda: 0.353954\n       Fn0: -5.103845 eV\n       Fn1: -0.405524 eV\n       Fn2: 1.112997 eV\n       Fn3: -3.585325 eV\n       F0: -5.14 eV\n       F1: 0.0 eV\n       F2: 1.640098 eV\n       F3: 0.221375 eV\n       eta: 0.848843 eV\n       Fo: -5.141526 eV\n</code></pre> <p>Above, the <code>compute_force</code> block is built upon the <code>johnson_force</code> operator that defines the interatomic potential as the Embedded-Atom Model from Johnson et al. parametrized for Tantalum. Additional interatomic potential are described in XXX.</p>"},{"location":"Beginner/Basics/ExampleInputDeck.html#input_data-block-definition","title":"<code>input_data</code> block definition","text":"<p>Finally, what would be a molecular dynamics simulation without atoms? The <code>input_data</code> block allows the user to create or insert particles in the simulation domain. </p> <pre><code>   input_data:\n     - init_rcb_grid\n     - lattice:\n         structure: BCC\n         types: [ Ta, Ta ]\n         size: [ 3.3 ang, 3.3 ang, 3.3 ang ]\n</code></pre> <p>That <code>input_data</code> block first applies the <code>init_rcb_grid</code> operators that distributes the simulation domain cells on the different processors so the particles can be appropriately distributed. It then calls the <code>lattice</code> operator that replicates a BCC unit cell of Tantalum with lattice parameter equal to 3.3 ang in the entire domain. Whether the final simulation cell is commensurized with the minimal BCC unit cell defined above strictly depends on the way the domain was defined. Other ways to create the particles can be found in XXX.</p>"},{"location":"Beginner/Basics/ExampleInputDeck.html#running-the-simulation","title":"Running the simulation","text":""},{"location":"Beginner/Basics/ExampleInputDeck.html#full-input-deck","title":"Full input deck","text":"<p>Gathering all the YAML blocks above, the final input decks is defined as below:</p> <pre><code>   global:\n     timestep: 0                      \n     physical_time: 0.                \n     dt: 1.0e-3 ps                    \n     rcut_inc: 1.0 ang                \n     log_mode: default                \n     simulation_log_frequency: 10     \n     simulation_end_iteration: 10000  \n\n   domain:\n     cell_size: 6.6 ang\n     grid_dims: [10, 10, 10]\n     bounds: [[ 0.0 ang,  0.0 ang,  0.0 ang],\n              [66.0 ang, 66.0 ang, 66.0 ang]]\n     periodic: [true,true,true]\n     expandable: false\n\n   species:\n     - Ta:\n         mass: 180.95 Da\n         z: 73\n         charge: 0 e-\n\n   compute_force: johnson_force\n\n   johnson_force:\n     rcut: 6.1 ang\n     parameters:\n       re: 2.860082 ang\n       fe: 3.08634 eV/ang\n       rhoe: 33.787168 eV/ang\n       alpha: 8.489528\n       beta: 4.527748\n       A: 0.611679 eV\n       B: 1.032101 eV\n       kappa: 0.176977\n       lambda: 0.353954\n       Fn0: -5.103845 eV\n       Fn1: -0.405524 eV\n       Fn2: 1.112997 eV\n       Fn3: -3.585325 eV\n       F0: -5.14 eV\n       F1: 0.0 eV\n       F2: 1.640098 eV\n       F3: 0.221375 eV\n       eta: 0.848843 eV\n       Fo: -5.141526 eV\n\n   input_data:\n     - init_rcb_grid\n     - lattice:\n         structure: BCC\n         types: [ Ta, Ta ]\n         size: [ 3.3 ang, 3.3 ang, 3.3 ang ]\n</code></pre>"},{"location":"Beginner/Basics/ExampleInputDeck.html#running-the-case","title":"Running the case","text":"<p>Copy-pasting the above YAML structure into a file named <code>tantalum_nve.msp</code>, you should be able to run the case by using the following commands in a terminal:</p> <pre><code>source ${XSP_INSTALL_DIR}/bin/setup-env.sh\nOMP_NUM_THREADS=20\nonika-exec input_deck.msp\n</code></pre> <p>When running this case, you'll notice that multiple files are created. First, a <code>thermodynamic_state.csv</code> file is written to disk with information every 10 steps. This is because in the <code>global</code> clock, the default value for <code>simulation_dump_thermo_frequency</code> is se to 10. In addition, multiple <code>*.MpiIO</code> files have been created and are the consequence of the <code>simulation_dump_frequency</code> set to 1000. The later triggers the <code>dump_data</code> operation that outputs by default a restart file in the <code>exaStamp</code> format. A detailed explanation of YAML configuration and default parameters is provided in the next section.</p>"},{"location":"Beginner/Examples/index.html","title":"exaStamp examples","text":""},{"location":"Beginner/Examples/Example1.html","title":"Example 1","text":""},{"location":"Beginner/Examples/Example2.html","title":"Example 2","text":""},{"location":"Beginner/Examples/Example3.html","title":"Example 3","text":""},{"location":"Beginner/Tutorials/index.html","title":"exaStamp tutorials","text":""},{"location":"Beginner/Tutorials/Tutorial1.html","title":"Tutorial 1","text":""},{"location":"Beginner/Tutorials/Tutorial2.html","title":"Tutorial 2","text":""},{"location":"Beginner/Tutorials/Tutorial3.html","title":"Tutorial 3","text":""},{"location":"BuildInstall/index.html","title":"Installation guide","text":"<p><code>exaStamp</code> is written in <code>C++17</code>/<code>C++20</code>. It can be built under Linux using either <code>CMake</code> or <code>Spack</code>. The <code>Spack</code> installation is recommended for users who do not want to develop in <code>exaStamp</code>. However, we provide extensive installation instructions for HPC clusters through the <code>CMake</code> procedure only.</p> <p>Installing <code>exaStamp</code> therefore consists of first installing <code>onika</code> and <code>exaNBody</code> that both depend on <code>YAML</code> as <code>exaStamp</code>. The <code>Spack</code> manager is only required if you do not want to install <code>exaStamp</code> through <code>CMake</code>. To allow for a clean and simple installation of <code>exaStamp</code>, we also provide the installation instructions for the required packages.</p> <p>A basic procedure is provided for a Spack installation as well as a CMake installation from the sources.</p>"},{"location":"BuildInstall/cmake_installation.html","title":"Installation with CMake","text":"<p><code>exaStamp</code> installation first consists in building both the <code>ONIKA</code> HPC platform and the <code>exaNBody</code> particle simulation framework. Below are instructions for building both as well as final instruction for building <code>exaStamp</code>. Please note that the required minimal <code>CMake</code> version is <code>3.26</code>. </p>"},{"location":"BuildInstall/cmake_installation.html#minimal-requirements","title":"Minimal requirements","text":""},{"location":"BuildInstall/cmake_installation.html#yaml-library","title":"YAML library","text":"<p>All three platforms extensively use the <code>YAML</code> Library. To build <code>YAML</code> from sources, read the following instructions. Installations procedures using <code>spack</code>, <code>apt-get</code> or <code>CMake</code> are provided.</p> <p>Installation procedure for YAML</p> CMakeSpackapt-get install <p><pre><code>def funct(a,b):\n    return 0\n</code></pre> <pre><code># Retrieve YAML sources into temporary folder\nYAMLTMPFOLDER=${path_to_tmp_yaml}\nmkdir ${YAMLTMPFOLDER} &amp;&amp; cd ${YAMLTMPFOLDER}\ngit clone --depth 1 --branch yaml-cpp-0.6.3 git@github.com:jbeder/yaml-cpp.git\n\n# Define installation directory\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3\n\n# Build and install YAML from sources using CMake \ncd ${YAMLTMPFOLDER} &amp;&amp; mkdir build &amp;&amp; cd build\ncmake -DCMAKE_BUILD_TYPE=Debug \\\n      -DCMAKE_INSTALL_PREFIX=${YAML_CPP_INSTALL_DIR} \\\n      -DYAML_BUILD_SHARED_LIBS=OFF \\\n      -DYAML_CPP_BUILD_CONTRIB=ON \\\n      -DYAML_CPP_BUILD_TESTS=OFF \\\n      -DYAML_CPP_BUILD_TOOLS=OFF \\\n      -DYAML_CPP_INSTALL=ON \\\n      -DCMAKE_CXX_FLAGS=-fPIC \\\n      ../yaml-cpp\nmake -j4 install\nexport YAML_CPP_INSTALL_DIR=${YAML_CPP_INSTALL_DIR}/yaml-cpp-0.6.3\n# Remove temporary folder\ncd ../..\nrm -r ${YAMLTMPFOLDER}            \n</code></pre></p> <pre><code>spack install yaml-cpp@0.6.3\nspack load yaml-cpp@0.6.3\n</code></pre> <pre><code>sudo apt-get install libyaml-cpp-dev\n</code></pre> <p>At this point, you should have YAML installed on your system. Please note that the installation procedure of YAML from sources using <code>CMake</code> also works on HPC clusters. In the following, remember to add the `-DCMAKE_PREFIX_PATH=${YAML_CPP_INSTALL_DIR} argument to your cmake command.</p>"},{"location":"BuildInstall/cmake_installation.html#onika","title":"Onika","text":"<p><code>ONIKA</code> (Object Network Interface for Knit Applications), is a component based HPC software platform to build numerical simulation codes. It is the foundation for the <code>exaNBody</code> particle simulation platform but is not bound to N-Body problems nor other domain specific simulation code. Existing applications based on its building blocks include Molecular Dynamics, particle based fluid simulations using methods such as Smooth Particle Hydrodynamics (SPH) or rigid body simulations using methods such as Discrete Element Method (DEM). It uses industry grade standards and widely adopted technologies such as CMake and C++20 for development and build, <code>YAML</code> for user input files, MPI and OpenMP for parallel programming, Cuda and HIP for GPU acceleration. To build <code>ONIKA</code> from sources, read the following instructions.</p> <p>Installation procedure for ONIKA</p> UBUNTU CPUUBUNTU GPURhel x INTEL x CUDARhel x GCC x CUDA <pre><code># Adapt depending on where you want to download onika\ncd ${HOME}/dev\ngit clone git@github.com:Collab4exaNBody/onika.git\nONIKA_SRC_DIR=${HOME}/dev/onika\nONIKA_INSTALL_DIR=${HOME}/local/onika            \nmkdir build_onika &amp;&amp; cd build_onika\n\nONIKA_SETUP_ENV_COMMANDS=\"\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=OFF \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j4 install\n</code></pre> <pre><code># Adapt depending on where you want to download onika\ncd ${HOME}/dev\ngit clone git@github.com:Collab4exaNBody/onika.git\nONIKA_SRC_DIR=${HOME}/dev/onika\nONIKA_INSTALL_DIR=${HOME}/local/onika            \nmkdir build_onika &amp;&amp; cd build_onika\n\nONIKA_SETUP_ENV_COMMANDS=\"\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=ON \\\n      -DCMAKE_CUDA_COMPILER=${PATH_TO_NVCC} \\\n      -DCMAKE_CUDA_ARCHITECTURES=${ARCH} \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j4 install\n</code></pre> <pre><code>ONIKA_INSTALL_DIR=${HOME}/local/onika\nONIKA_SRC_DIR=${HOME}/dev/onika\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3/lib/cmake/yaml-cpp            \nONIKA_SETUP_ENV_COMMANDS=\"module purge ; module load gnu/11.2.0 nvhpc/24.3 inteloneapi/24.2.0 mpi/openmpi cmake/3.26.4\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\nCXX_COMPILER=`which icpx`\nC_COMPILER=`which icx`\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -DCMAKE_C_COMPILER=${C_COMPILER} \\\n      -DCMAKE_CXX_COMPILER=${CXX_COMPILER} \\\n      -DCMAKE_CXX_FLAGS=-diag-disable=15518,15552 \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=ON \\\n      -DCMAKE_CUDA_COMPILER=/ccc/products/cuda-12.4/system/default/bin/nvcc \\\n      -DCMAKE_CUDA_FLAGS=\"-ccbin ${CXX_COMPILER} -allow-unsupported-compiler\" \\\n      -DCMAKE_CUDA_ARCHITECTURES=80 \\\n      -DONIKA_MPIRUN_CMD=\"/usr/bin/ccc_mprun\" \\\n      -DMPIEXEC_EXECUTABLE=`which mpiexec` \\\n      -DMPIEXEC_MAX_NUMPROCS=32 \\\n      -DMPIEXEC_NUMCORE_FLAG=\"-c\" \\\n      -DMPIEXEC_NUMPROC_FLAG=\"-n\" \\\n      -DMPIEXEC_PREFLAGS=\"-pa100-bxi\" \\\n      -DMPIEXEC_PREFLAGS_DBG=\"-pa100-bxi;-Xall;xterm;-e\" \\\n      -DONIKA_ALWAYS_USE_MPIRUN=ON \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j32 install\n</code></pre> <pre><code>ONIKA_INSTALL_DIR=${HOME}/local/onika\nONIKA_SRC_DIR=${HOME}/dev/onika\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3/lib/cmake/yaml-cpp\nONIKA_SETUP_ENV_COMMANDS=\"module purge ; module load gnu/12.3.0 nvhpc/24.3 mpi/openmpi cmake/3.26.4\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=ON \\\n      -DCMAKE_CUDA_COMPILER=/ccc/products/cuda-12.4/system/default/bin/nvcc \\\n      -DCMAKE_CUDA_ARCHITECTURES=80 \\\n      -DONIKA_MPIRUN_CMD=\"/usr/bin/ccc_mprun\" \\\n      -DMPIEXEC_EXECUTABLE=`which mpiexec` \\\n      -DMPIEXEC_MAX_NUMPROCS=32 \\\n      -DMPIEXEC_NUMCORE_FLAG=\"-c\" \\\n      -DMPIEXEC_NUMPROC_FLAG=\"-n\" \\\n      -DMPIEXEC_PREFLAGS=\"-pa100-bxi\" \\\n      -DMPIEXEC_PREFLAGS_DBG=\"-pa100-bxi;-Xall;xterm;-e\" \\\n      -DONIKA_ALWAYS_USE_MPIRUN=ON \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j32 install                \n</code></pre>"},{"location":"BuildInstall/cmake_installation.html#exanbody","title":"exaNBody","text":"<p><code>exaNBody</code> is a software platform to build-up numerical simulations solving N-Body like problems. Typical applications include Molecular Dynamics, particle based fluid simulations using methods such as Smooth Particle Hydrodynamics (SPH) or rigid body simulations using methods such as Discrete Element Method (DEM). It uses standard and widely adopted technologies such as C++20, YAML, OpenMP , Cuda or HIP. When installing <code>exaNBody</code>, first sourcing the <code>ONIKA</code> environment will automatically update whether CUDA is activated or not.</p> <p>Installation procedure for exaNBody</p> UBUNTURhel x INTELRhel x GCC <pre><code># Adapt depending on where you want to download ``exaNBody``\ncd ${HOME}/dev\ngit clone -b release-2.0 git@github.com:Collab4exaNBody/exaNBody.git\nXNB_SRC_DIR=${HOME}/dev/exaNBody\nXNB_INSTALL_DIR=${HOME}/local/exaNBody\nmkdir build_exaNBody &amp;&amp; cd build_exaNBody\n\nsource ${ONIKA_INSTALL_DIR}/bin/setup-env.sh\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XNB_INSTALL_DIR} \\\n      -Donika_DIR=${ONIKA_INSTALL_DIR} \\\n      ${XNB_SRC_DIR}\nmake -j4 install\n</code></pre> <pre><code># Adapt depending on where you want to download ``exaNBody``\ncd ${HOME}/dev\ngit clone -b release-2.0 git@github.com:Collab4exaNBody/exaNBody.git\nXNB_SRC_DIR=${HOME}/dev/exaNBody\nXNB_INSTALL_DIR=${HOME}/local/exaNBody\nmkdir build_exaNBody &amp;&amp; cd build_exaNBody\n\nsource ${ONIKA_INSTALL_DIR}/bin/setup-env.sh\nCXX_COMPILER=`which icpx`\nC_COMPILER=`which icx`\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XNB_INSTALL_DIR} \\\n      -DCMAKE_C_COMPILER=${C_COMPILER} \\\n      -DCMAKE_CXX_COMPILER=${CXX_COMPILER} \\\n      -DCMAKE_CXX_FLAGS=-diag-disable=15518,15552 \\\n      -Donika_DIR=${ONIKA_INSTALL_DIR} \\\n      ${XNB_SRC_DIR}    \nmake -j32 install\n</code></pre> <pre><code>XNB_INSTALL_DIR=${HOME}/local/exaNBody\nsource ${ONIKA_INSTALL_DIR}/bin/setup-env.sh\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XNB_INSTALL_DIR} \\\n      -Donika_DIR=${ONIKA_INSTALL_DIR} \\\n      ${XNB_SRC_DIR}\nmake -j32 install\n</code></pre>"},{"location":"BuildInstall/cmake_installation.html#exastamp-installation","title":"exaStamp installation","text":"<p>When installing <code>exaStamp</code>, first sourcing the <code>exaNBody</code> environment will automatically update whether CUDA is activated or not.</p> <p>Installation procedure for exaStamp</p> UBUNTURhel x INTELRhel x GCC <pre><code># Adapt depending on where you want to download ``exaStamp``\ncd ${HOME}/dev\ngit clone -b exaNBody-release-2.0 git@github.com:Collab4exaNBody/exaStamp.git\nXSP_SRC_DIR=${HOME}/dev/exaStamp\nmkdir build_exaStamp &amp;&amp; build_exaStamp\n\nXSP_INSTALL_DIR=${HOME}/local/exaStamp\nsource ${XNB_INSTALL_DIR}/bin/setup-env.sh\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XSP_INSTALL_DIR} \\\n      -DexaNBody_DIR=${XNB_INSTALL_DIR} \\\n      ${XSP_SRC_DIR}\nmake -j4 install\nsource ${XSP_INSTALL_DIR}/bin/setup-env.sh\n</code></pre> <pre><code>XSP_INSTALL_DIR=${HOME}/local/exaStamp\nsource ${XNB_INSTALL_DIR}/bin/setup-env.sh\nCXX_COMPILER=`which icpx`\nC_COMPILER=`which icx`\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XSP_INSTALL_DIR} \\\n      -DCMAKE_C_COMPILER=${C_COMPILER} \\\n      -DCMAKE_CXX_COMPILER=${CXX_COMPILER} \\\n      -DCMAKE_CXX_FLAGS=-diag-disable=15518,15552 \\\n      -DexaNBody_DIR=${XNB_INSTALL_DIR} \\\n      ${XSP_SRC_DIR}        \nmake -j32 install\nsource ${XSP_INSTALL_DIR}/bin/setup-env.sh      \n</code></pre> <pre><code>XNB_INSTALL_DIR=${HOME}/local/exaNBody\nsource ${XNB_INSTALL_DIR}/bin/setup-env.sh\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XSP_INSTALL_DIR} \\\n      -DexaNBody_DIR=${ONIKA_INSTALL_DIR} \\\n      ${XSP_SRC_DIR}\nmake -j32 install\nsource ${XSP_INSTALL_DIR}/bin/setup-env.sh      \n</code></pre>"},{"location":"BuildInstall/running.html","title":"Running your simulation","text":"<p>Now that you have installed <code>onika</code>, <code>exaNBody</code> and <code>exaStamp</code>, you can create your simulation file using the <code>YAML</code> format. Please refer to the <code>Beginner's guide to exaStamp</code> section to learn how to build your first input deck! Once this file is constructed, you can run your simulation with a specified number of <code>MPI</code> processes and threads per <code>MPI</code> process. </p>"},{"location":"BuildInstall/running.html#simple-yaml-input-file","title":"Simple YAML input file","text":"<p>Below is an example of an <code>YAML</code> input file that allows to:</p> <ul> <li>Define a 3D-periodic simulation cell containing an FCC Copper sample</li> <li>Assign a gaussian random noise to particles positions and velocities</li> <li>Performs the time integration using a Velocity-Verlet scheme</li> <li>Uses a Langevin thermostat to maintain the temperature at 300. K</li> <li>Dumps <code>.xyz</code> files at a specific frequency for vizualization with <code>OVITO</code></li> </ul> <p>Basic YAML example for exaStamp</p> <pre><code># Choose the grid flavor\ngrid_flavor: grid_flavor_full\n\n# Define the species present in the system\nspecies:\n  - Cu: { mass: 63.546 Da , z: 29 , charge: 0.0 e- }\n\n# Interatomic potential\nsutton_chen_force:\n  rcut: 7.29 ang\n  parameters:\n    c: 3.317E+01\n    epsilon: 3.605E-21 J\n    a0: 0.327E-09 m\n    n: 9.050E+00\n    m: 5.005E+00\n\n# Force operator (interatomic potential + Langevin thermostat)\ncompute_force:\n  - sutton_chen_force\n  - langevin_thermostat: { T: 300. K, gamma: 0.1 ps^-1 }\n\n# System's creation\ninput_data:\n  - domain:\n      cell_size: 5.0 ang\n  - bulk_lattice:\n      structure: FCC\n      types: [ Cu, Cu, Cu, Cu]\n      size: [ 3.615 ang , 3.615 ang , 3.615 ang ]\n      repeat: [ 20, 20, 20 ]\n  - gaussian_noise_r:\n  - gaussian_noise_v:\n\n# Simulation parameters        \nglobal:\n  simulation_end_iteration: 1000\n  simulation_log_frequency: 20\n  simulation_dump_thermo_frequency: -1\n  simulation_dump_frequency: -1\n  rcut_inc: 1.0 ang\n  dt: 2.0e-3 ps\n  init_temperature: 5. K\n</code></pre>"},{"location":"BuildInstall/running.html#its-go-time","title":"It's go time","text":"<p>To run the above case, and depending if you installed <code>exaStamp</code> using CMake or spack follow these instructions:</p> <p>Running an exaStamp simulation</p> CMakeSpack <pre><code>source ${XSP_BUILD_DIR}/exaStamp\nexport OMP_NUM_THREADS=10\nexport N_MPI=2\nmpirun -np ${N_MPI} ${XSP_BUILD_DIR}/exaStamp myinput.msp\n</code></pre> <pre><code>export OMP_NUM_THREADS=10\nexport N_MPI=2\nspack load exastamp\nmpirun -np ${N_NMPI} exaStamp myinput.msp\n</code></pre>"},{"location":"BuildInstall/spack_installation.html","title":"Installation with Spack","text":"<p>Installation with <code>Spack</code> is easy and preferable for users who don't want to develop in <code>exaStamp</code>. Only stable versions are added when you install <code>exaStamp</code> with <code>Spack</code>, meaning that it doesn't provide you access to the development branches. In addition, the main branch of <code>exaStamp</code> will never be directly accessible via this installation method.</p>"},{"location":"BuildInstall/spack_installation.html#minimal-requirements","title":"Minimal requirements","text":""},{"location":"BuildInstall/spack_installation.html#spack-package-manager","title":"Spack package manager","text":"<p>Below are instructions to first retrieve spack sources and install it on your system. First, clone the corresponding git repository and source the appropriate environment.</p> <p>Clone Spack</p> <pre><code>cd ${HOME}/dev\ngit clone https://github.com/spack/spack.git\nexport SPACK_ROOT=${HOME}/dev/spack\nsource ${SPACK_ROOT}/share/spack/setup-env.sh\n</code></pre>"},{"location":"BuildInstall/spack_installation.html#yaml-library","title":"YAML library","text":"<p>All three platforms extensively use the <code>YAML</code> Library. To build <code>YAML</code> from sources, read the following instructions. Installations procedures using <code>spack</code>, <code>apt-get</code> or <code>CMake</code> are provided.</p> <p>Installation procedure for YAML</p> Spackapt-get installCMake <pre><code>spack install yaml-cpp@0.6.3\nspack load yaml-cpp@0.6.3\n</code></pre> <pre><code>sudo apt-get install libyaml-cpp-dev\n</code></pre> <pre><code># Retrieve YAML sources into temporary folder\nYAMLTMPFOLDER=${path_to_tmp_yaml}\nmkdir ${YAMLTMPFOLDER} &amp;&amp; cd ${YAMLTMPFOLDER}\ngit clone --depth 1 --branch yaml-cpp-0.6.3 git@github.com:jbeder/yaml-cpp.git\n\n# Define installation directory\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3\n\n# Build and install YAML from sources using CMake \ncd ${YAMLTMPFOLDER} &amp;&amp; mkdir build &amp;&amp; cd build\ncmake -DCMAKE_BUILD_TYPE=Debug \\\n      -DCMAKE_INSTALL_PREFIX=${YAML_CPP_INSTALL_DIR} \\\n      -DYAML_BUILD_SHARED_LIBS=OFF \\\n      -DYAML_CPP_BUILD_CONTRIB=ON \\\n      -DYAML_CPP_BUILD_TESTS=OFF \\\n      -DYAML_CPP_BUILD_TOOLS=OFF \\\n      -DYAML_CPP_INSTALL=ON \\\n      -DCMAKE_CXX_FLAGS=-fPIC \\\n      ../yaml-cpp\nmake -j4 install\nexport YAML_CPP_INSTALL_DIR=${YAML_CPP_INSTALL_DIR}/yaml-cpp-0.6.3\n# Remove temporary folder\ncd ../..\nrm -r ${YAMLTMPFOLDER}            \n</code></pre> <p>At this point, you should have YAML installed on your system. Please note that the installation procedure of YAML from sources using <code>CMake</code> also works on HPC clusters. In the following, remember to add the `-DCMAKE_PREFIX_PATH=${YAML_CPP_INSTALL_DIR} argument to your cmake command.</p>"},{"location":"BuildInstall/spack_installation.html#exastamp-installation","title":"exaStamp installation","text":"<p>First, clone the <code>spack-repos</code> GitHub repository on your computer and add this repository to spack. This repository contains <code>onika</code>, <code>exaNBody</code> and <code>exaStamp</code> recipes that allow for their installation.</p> <p>Clone spack-repos</p> <pre><code>git clone https://github.com/Collab4exaNBody/spack-repos.git\nspack repo add spack-repos\n</code></pre> <p>Then, simply install <code>exaStamp</code> using the following command.</p> <p>Install exaStamp</p> <pre><code>spack install exastamp\n</code></pre> <p>If you have a GPU on your machine, you can also ask for a CUDA installation through the following command:</p> <p>Install exaStamp with CUDA support</p> <pre><code>spack install exastamp+cuda\n</code></pre> <p>The default version will be the latest stable release. However, you can also ask for a specific branch as follow:</p> <p>Install exaStamp specific version</p> <pre><code>spack install exastamp@3.7.2\nspack install exastamp@3.7.0\n</code></pre> <p>Finally, the commands listed above will install the appropriate version of <code>cmake</code>, <code>yaml-cpp</code>, <code>onika</code> and <code>exaNBody</code> and additional required packages by the <code>exaStamp</code> recipe. Eventually, to run an <code>exaStamp</code> case, do the following:</p> <p>Install exaStamp</p> <pre><code>spack load exastamp\nexaStamp myinput.msp\n</code></pre>"},{"location":"References/index.html","title":"Citing exaStamp","text":""},{"location":"References/contributions.html","title":"Contribution papers","text":"<p>exaStamp is the result of many years of co-development and contains previously published as well as independently developed methods and algorithms initially designed as post analysis tools for some of them. Below is a table of various methods available in exaStamp with corresponding paper that should be cited when used in the context of your work.</p> Operator Description Reference <code>cc_label</code> Connected component labelling <sup>1</sup>Publication <code>compute_local_slip_system</code> Slip/Twin system identification <sup>2</sup>Publication <code>xform_function</code> Deformation path formalism Publication <ol> <li> <p>Killian Babilotte, Aliz\u00e9e Dubois, Thierry Carrard, Paul Lafourcade, Laurent Videau, Jean-Fran\u00e7ois Molinari, and Laurent Soulard. On-the-fly clustering for exascale molecular dynamics simulations. Computer Physics Communications, 307:109427, 2025. URL: https://www.sciencedirect.com/science/article/pii/S0010465524003503, doi:https://doi.org/10.1016/j.cpc.2024.109427.\u00a0\u21a9</p> </li> <li> <p>Paul Lafourcade, Guillaume Ewald, Thierry Carrard, and Christophe Denoual. Extraction of slip systems and twinning variants from a lagrangian analysis of molecular dynamics simulations. Mechanics of Materials, 200:105189, 2025. URL: https://www.sciencedirect.com/science/article/pii/S0167663624002813, doi:https://doi.org/10.1016/j.mechmat.2024.105189.\u00a0\u21a9</p> </li> </ol>"},{"location":"References/reference.html","title":"Reference papers","text":"<p>A reference paper for exaStamp is currently under preparation. It will provide an overview of the code including a description of its parallelism, features, performance with some focus on specific developments it includes.</p> <p>exaStamp : a portable HPC solution on GPU architectures for MD simulations of shock physics</p> <pre><code># Code block content\n</code></pre> <pre><code>@software{exastamp2025,\n  author  = {Your Name and Collaborator Name},\n  title   = {exaStamp : a portable HPC solution on GPU architectures for MD simulations of shock physics},\n  year    = {2025},\n  version = {3.7.2},\n  url     = {https://github.com/Collab4exaNBody/exaStamp},\n  license = {Apache-2.0}\n}\n</code></pre>"},{"location":"User/index.html","title":"User guide to exaStamp","text":""},{"location":"User/DomainRegions/index.html","title":"Simulation's graph","text":"<p>The default simulation sequence of <code>exaStamp</code> is defined in the <code>main-config.msp</code> file located in <code>exaStamp/data/config</code>. In particular, the <code>simulation</code> block allows for the entire parametrization of the MD simulation, from the initial information output and hardware initialization to the main MD loop and hardware finalization.</p> <p>Default YAML block for an exaStamp simulation</p> <pre><code>simulation:\n  name: MySimulation\n  body:\n    - print_logo_banner                                   # Print the exaStamp banner\n    - hw_device_init                                      # Default communicator + CUDA initialization\n    - make_empty_grid                                     # Create an empty grid\n    - grid_flavor                                         # Define what information is attached to the grid\n    - global                                              # Global simulation controls\n    - init_parameters                                     # Additional control parameters\n    - generate_default_species                            # Generate default species\n    - particle_regions                                    # Geometrical regions definition\n    - preinit_rcut_max                                    # Automatic cell_size calculation\n    - domain                                              # Simulation domain definition\n    - init_prolog                                         # Initialization prologue\n    - input_data                                          # Populate domain with particles\n    - species:                                            # Species definition\n        verbose: false\n        fail_if_empty: true\n    - grid_post_processing                                # Grid memory compaction\n    - reduce_species_after_read                           # Update particle species\n    - init_rcut_max                                       # Update neighborhood distance and displacement tolerance\n    - print_domain                                        # Print Domain information\n    - performance_adviser: { verbose: true }              # Print performance advices\n    - do_init_temperature                                 # Initialize temperature if needed\n    - init_epilog                                         # Initialization epilogue\n    - species:                                            # Species definition recheck\n        verbose: true\n        fail_if_empty: true\n    - first_iteration                                     # Simulation first iteration\n    - compute_loop                                        # Simulation compute loop\n    - simulation_epilog                                   # Simulation finalization\n    - hw_device_finalize                                  # CUDA finalization\n</code></pre> <p>In this YAML block, some operators are mandatory. We provide in the next section a input deck example with the minimal information required. In addition, some of the YAML operators defined in the <code>simulation</code> block are defined in other configuration files located in <code>exaStamp/data/config</code>. However, the mandatory blocks to be defined by the user to build a minimal input deck for exaStamp are:</p> <pre><code>   - global         # Global control of simulation parameters\n   - species        # Definition of the particles' species\n   - compute_force  # Choice of the interatomic potential\n   - domain         # Definition of the simulation's domain\n   - input_data     # Population of the domain with particles\n</code></pre> <p>The next section provides a basic example on how to build an <code>exaStamp</code> input deck with the minimal information required.</p> <pre><code>graph TD\n  A[\"`**Initialization**\n  Detect GPU Support\n  Create Grid\n  Preinit cutoff radius`\"]\n\n  B[Creating Particle Regions]\n\n  C[\"`**Input Data**\n  lattice, xyz, MpiIO\n  + field initialization`\"]\n\n  D[\"`**Firs Timestep**\n  Init partcles\n  - move\n  - migration\n  - neighbor lists\n  - compute forces`\"]\n\n  E[Time Loop];\n  F[test];\n  G[coucou];\n\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt; D;\n  D --&gt; E;\n  E --&gt; F;\n  F --&gt; G\n  G---&gt;|\"`t inf tfin`\"| D;\n\n</code></pre> <p>B --&gt;|Yes| C[Hmm...];   C --&gt; D[Debug];   D --&gt; B;   B ----&gt;|No| E[Yay!];   E ===&gt; newLines[\"<code>Line1                     Line 2                     Line 3</code>\"]  </p>"},{"location":"User/DomainRegions/domain.html","title":"Domain","text":"<p>.. _domain:</p>"},{"location":"User/DomainRegions/domain.html#domain","title":"Domain","text":""},{"location":"User/DomainRegions/domain.html#physical-space-vs-grid-space","title":"Physical space vs Grid space","text":"<p>In <code>exaStamp</code>, all the operations concerning parallelism are actually done in a denominated Grid space which is fully defined by the <code>domain</code> operator described hereafter. Before going into details about this operator, we need to describe how the simulation domain is defined. The 3D simulation box can be represented by its 3x3 frame matrix :math:<code>\\mathbf{H_P}</code> (with the subscript :math:<code>(\\cdot)_P</code> for physical space) built on the 3 periodicity vectors :math:<code>\\mathbf{a}</code>, :math:<code>\\mathbf{b}</code> and :math:<code>\\mathbf{c}</code>:</p> <p>.. math::</p> <p>\\mathbf{H_P} = \\begin{pmatrix} \\mathbf{a} | \\mathbf{b} | \\mathbf{c} \\end{pmatrix} = \\begin{pmatrix} a_x &amp; b_x &amp; c_x \\ a_y &amp; b_y &amp; c_y \\ a_z &amp; b_z &amp; c_z\\end{pmatrix}</p> <p>without any constraints on the periodicity vectors :math:<code>\\mathbf{a}</code>, :math:<code>\\mathbf{b}</code> and :math:<code>\\mathbf{c}</code> w.r.t the orthonormal frame. From this, we assume that the :math:<code>\\mathbf{H_P}</code> matrix can be decomposed as:</p> <p>.. math::</p> <p>\\mathbf{H_P} = \\mathbf{F_1} \\cdot \\mathbf{D} = \\mathbf{F_1} \\cdot \\begin{pmatrix} || \\mathbf{a} || &amp; 0 &amp; 0 \\ 0 &amp; || \\mathbf{b} || &amp; 0 \\ 0 &amp; 0 &amp; || \\mathbf{c} || \\end{pmatrix}</p> <p>where :math:<code>\\mathbf{D}</code> is a diagonal matrix with components equal to the norm of each periodicity vector and :math:<code>\\mathbf{F_1}</code> a transformation matrix that allows to transform the general (triclinic) physical domain to a pure orthorhombic unphysical one. :math:<code>\\mathbf{F_1}</code> can be trivially calculated as:</p> <p>.. math::</p> <p>\\mathbf{F_1} = \\mathbf{H_P} \\cdot \\mathbf{D}^{-1} = \\mathbf{H_P} \\cdot \\begin{pmatrix} \\frac{1}{ || \\mathbf{a} || } &amp; 0 &amp; 0 \\ 0 &amp; \\frac{1}{|| \\mathbf{b} ||} &amp; 0 \\ 0 &amp; 0 &amp; \\frac{1}{ || \\mathbf{c} || } \\end{pmatrix}</p> <p>In the Grid space, the domain needs to be defined through a diagonal matrix :math:<code>\\mathbf{H_G}</code> (with the subscript :math:<code>(\\cdot)_G</code> for grid space) where each diagonal component equals an integer multiple :math:<code>(n_x, n_y, n_z)</code>  of the cell_size :math:<code>c_s</code>:</p> <p>.. math::</p> <p>\\mathbf{H_G} = \\begin{pmatrix} n_x \\cdot c_s &amp; 0 &amp; 0 \\ 0 &amp; n_y \\cdot c_s &amp; 0 \\ 0 &amp; 0 &amp; n_z \\cdot c_s \\end{pmatrix}</p> <p>Finally, both physical space and grid space meet through the following equality:</p> <p>.. math::</p> <p>\\mathbf{H_G} = \\mathbf{X_f} \\cdot \\mathbf{H_P}</p> <p>which adds an additional constraint on the compatibility between the physical space and grid space. Indeed, for the compatibility to be satisfied, the diagonal matrix :math:<code>mathbf{D}</code> is mapped to the :math:<code>\\mathbf{H_G}</code> matrix through the following operation:</p> <p>.. math::</p> <p>\\mathbf{D} = \\mathbf{F_2} \\cdot \\mathbf{H_G}</p> <p>leading to:</p> <p>.. math::</p> <p>\\mathbf{F_2} = \\mathbf{D} \\cdot \\mathbf{H_G}^{-1} = \\begin{pmatrix} \\frac{||\\mathbf{a}||}{n_x \\cdot c_s} &amp; 0 &amp; 0 \\ 0 &amp; \\frac{||\\mathbf{b}||}{n_y \\cdot c_s} &amp; 0 \\ 0 &amp; 0 &amp; \\frac{||\\mathbf{c}||}{n_z \\cdot c_s} \\end{pmatrix}</p> <p>where :math:<code>(n_x,n_y,n_z)</code> and :math:<code>c_s</code> are fixed by the user as explained hereafter. If the user requires a specific cell size :math:<code>c_s</code>, then the number of cells and the appropriate :math:<code>\\mathbf{X_f}</code> are automatically calculated and vice versa. The final expression of the :math:<code>\\mathbf{X_f}</code> reads:</p> <p>.. math::</p> <p>\\mathbf{X_f} = \\mathbf{F_1} \\cdot \\mathbf{F_2} = \\mathbf{H_P} \\cdot \\mathbf{D}^{-1} \\cdot \\mathbf{D} \\cdot \\mathbf{H_G}^{-1} </p> <p>which simplifies to:</p> <p>.. math::</p> <p>\\mathbf{X_f} = \\mathbf{H_P} \\cdot \\mathbf{H_G}^{-1} = \\begin{pmatrix} \\mathbf{a} | \\mathbf{b} | \\mathbf{c} \\end{pmatrix} = \\begin{pmatrix} a_x &amp; b_x &amp; c_x \\ a_y &amp; b_y &amp; c_y \\ a_z &amp; b_z &amp; c_z\\end{pmatrix} \\cdot \\begin{pmatrix} n_x \\cdot c_s &amp; 0 &amp; 0 \\ 0 &amp; n_y \\cdot c_s &amp; 0 \\ 0 &amp; 0 &amp; n_z \\cdot c_s \\end{pmatrix}^{-1}</p> <p>which can lead to very simple expression of :math:<code>\\mathbf{X_f}</code> when for example the physical domain is a cubic domain with lengths exactly set to a multiple of cell size. The next section explains in detail how to fully define the domain.</p>"},{"location":"User/DomainRegions/domain.html#defining-the-domain","title":"Defining the domain","text":"<p>The <code>domain</code> operator</p> <p>The <code>domain</code> operator allows to fully define the simulation domain as follows:</p> <p>.. code-block:: yaml</p> <p>domain:      cell_size: 5.0 ang      grid_dims: [20,20,20]      bounds: [[0 ang ,0 ang,0 ang],[100 ang, 100 ang, 100 ang]]      xform: [[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]]      periodic: [true,true,true]      expandable: false</p> <p>where all properties of the <code>domain</code> block are described below.</p> <p>.. list-table::    :header-rows: 1</p> <ul> <li> <ul> <li>Property</li> <li>Description</li> <li>Data Type</li> <li>Default</li> </ul> </li> <li> <ul> <li><code>cell_size</code></li> <li>Grid cell size in grid space.</li> <li>float</li> <li>0.</li> </ul> </li> <li> <ul> <li><code>grid_dims</code></li> <li>3D Grid dimensions.</li> <li>IJK</li> <li>[0,0,0]</li> </ul> </li> <li> <ul> <li><code>bounds</code></li> <li>Domain bounds in grid space.</li> <li>AABB</li> <li>[[0,0,0], [0,0,0]]</li> </ul> </li> <li> <ul> <li><code>xform</code></li> <li>Grid space to real space transformation matrix.</li> <li>Mat3d</li> <li>[[1,0,0],[0,1,0],[0,0,1]]</li> </ul> </li> <li> <ul> <li><code>periodic</code></li> <li>Periodic boundary conditions.</li> <li>sequence</li> <li>[true, true, false]</li> </ul> </li> <li> <ul> <li><code>mirror</code></li> <li>Mirror boundary conditions.</li> <li>sequence</li> <li>[]</li> </ul> </li> <li> <ul> <li><code>expandable</code></li> <li>Domain expandability.</li> <li>bool</li> <li>true</li> </ul> </li> </ul> <p>.. warning::</p> <p>When defining the simulation domain through this operator, all properties must be consistent with each other. In particular, <code>cell_size</code> multiplied by <code>grid_dims</code> must be equal to max(<code>bounds</code>) - min(<code>bounds</code>).</p> <p>Usage examples</p> <p>Multiple examples of domain definitions are provided below with, for each case, an example of the <code>domain</code> YAML block, a visualization of the physical space and another visualization of the grid space.</p> <p>Cubic domain ^^^^^^^^^^^^</p> <p>The first example creates a cubic physical domain with 100 :math:<code>\\AA</code> side length, with 20 cells in each direction. In grid space, the domain also is cubic with the same dimensions.</p> <p>.. code-block:: YAML</p> <p>domain:      cell_size: 5.0 ang      grid_dims: [20,20,20]      bounds: [[0 ang ,0 ang,0 ang],[100 ang, 100 ang, 100 ang]]      xform: [[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]]      periodic: [true,true,true]      expandable: false   </p> <p>In that case, the :math:<code>\\mathbf{X_f}</code> matrix equal the identity matrix and the grid space domain is exactly equal to the physical space domain. Below are displayed the 3D physical (left) and grid (right) domains look like:</p> <p>.. figure:: /_static/cubic_both_spaces.png    :width: 600pt    :align: center</p> <p>Orthorhombic domain ^^^^^^^^^^^^^^^^^^^</p> <p>In that second example, an orthorhombic physical domain with 80 :math:<code>\\AA</code>, 100 :math:<code>\\AA</code> and 120 :math:<code>\\AA</code> side lengths is created, with 16, 20 and 25 cells in each direction. In grid space, the domain is also orthorhombic with the same dimensions since the physical size exactly equals a finite number of cells in each direction.</p> <p>.. code-block:: yaml</p> <p># 1st solution    domain:      cell_size: 5.0 ang      grid_dims: [16,20,24]      bounds: [[0 ang ,0 ang,0 ang],[80 ang, 100 ang, 120 ang]]      xform: [[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]]      periodic: [true,true,true]      expandable: false</p> <p>As before, since the physical domain exactly equals (in each direction), a finite number of cells, the grid domain has the exact same dimensions.</p> <p>.. figure:: /_static/ortho1_both_spaces.png    :width: 600pt    :align: center</p> <p>If for some reasons the user needs to have the same grid dimensions in each direction, it is possible to define an orthorhombic physical domain by modifying the :math:<code>\\mathbf{X_f}</code> matrix as follows:</p> <p>.. code-block:: yaml</p> <p># 2nd solution    domain:      cell_size: 5.0 ang      grid_dims: [20,20,20]      bounds: [[0 ang ,0 ang,0 ang],[100 ang, 100 ang, 100 ang]]      xform: [[0.8,0.,0.],[0.,1.,0.],[0.,0.,1.2]]      periodic: [true,true,true]      expandable: false</p> <p>This way, the physical domain has the exact same dimensions as before, but the grid domain is now cubic with 20 cells in each direction.</p> <p>.. figure:: /_static/ortho2_both_spaces.png    :width: 600pt    :align: center</p> <p>Restricted triclinic domain ^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>.. code-block:: yaml</p> <p># 1st solution: restricted triclinic    # (e.g. a is parallel to x and    # b is in the (x,y) plane)    domain:      cell_size: 5.0 ang      grid_dims: [20,20,20]      bounds: [[0 ang ,0 ang,0 ang],[100 ang, 100 ang, 100 ang]]      xform: [[1.,0.1,0.2],[0.,1.,0.2],[0.,0.,1.]]      periodic: [true,true,true]      expandable: false</p> <p>.. figure:: /_static/restricted_tri_both_spaces.png    :width: 600pt    :align: center</p> <p>Generalized triclinic domain ^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>.. code-block:: yaml</p> <p># 2nd solution: general triclinic    # (e.g. no constraints on a or b)            domain:      cell_size: 5.0 ang      grid_dims: [20,20,20]      bounds: [[0 ang ,0 ang,0 ang],[100 ang, 100 ang, 100 ang]]      xform: [[1.,0.05,0.1],[0.05,1.,0.1],[0.1,0.1,1.2]]      periodic: [true,true,true]      expandable: false</p> <p>.. figure:: /_static/generalized_tri_both_spaces.png    :width: 600pt    :align: center</p>"},{"location":"User/DomainRegions/domain.html#alternative-ways-for-defining-the-domain","title":"Alternative ways for defining the domain","text":"<p>The <code>domain_from_lengths_angles</code> operator</p> <p>Built-in particles creators</p> <p>External file readers</p> <p>In some cases, the simulation domain does not need to be fully defined as explained above. Indeed, the domain information can sometimes already be contained in external files or fully defined by the material the user needs to model. Below is a list of situations where the domain is fully or partially defined. Additional details can be found in the corresponding documentation sections.</p> <ul> <li><code>bulk_lattice</code>: The system shape and size is created according to the replication in the 3D space of a unit cell chosen by the user. See :ref:<code>input-bulk-lattice</code>.</li> <li><code>read_xyz_file_with_xform</code>: Instead of creating the system from a template, an external <code>.xyz</code> file is read in which the number of atoms, their positions and the simulation cell size and shape ir provided. In that case, only the <code>cell_size</code> property of the <code>domain</code> YAML block is needed. See :ref:<code>input-read-xyz-xform</code>.</li> <li><code>read_dump_atoms</code>: The simulation starts at a specific timestep for which a restart file was generated. That restart files usually contains all information for the simulation domain. See :ref:<code>input-read-dump-atoms</code>.</li> <li><code>read_dump_molecule</code>: Same as above but for flexible molecules. See :ref:<code>input-read-dump-mol</code>.</li> <li><code>read_dump_rigidmol</code>: Same as above but for rigid molecules. See :ref:<code>input-read-dump-rigidmol</code>.</li> </ul>"},{"location":"User/DomainRegions/regions.html","title":"Regions","text":"<p>.. _regions:</p>"},{"location":"User/DomainRegions/regions.html#spatial-regions","title":"Spatial Regions","text":""},{"location":"User/DomainRegions/regions.html#geometrically-defined-regions","title":"Geometrically defined regions","text":"<p>The <code>particle_regions</code> operator</p> <p>Spatial regions can be very usefull in order to define areas in the simulation physical domain that are subsequently used to populate with particles or perform analysis on a subdomain for example. One or multiple regions can be defined using the <code>particle_regions</code> YAML block:</p> <p>.. code-block:: yaml</p> <p>particles_regions:      - REG1      - REG2      - REG3</p> <p>where <code>REG1</code>, <code>REG2</code> and <code>REG3</code> can be regions defined using different ways. The next section presents a list of different ways that can be used to define a spatial region.</p> <p>.. warning::</p> <p>All regions are defined in the physical space, not in the grid space. This is to be taken into account when creating regions, especially when dealing with triclinic physical domains.</p> <p>Individual regions</p> <p>Parallelepiped ^^^^^^^^^^^^^^</p> <p>.. list-table:: Parallelepiped regions     :widths: 50 50    :header-rows: 0</p> <ul> <li> <ul> <li> <p>.. code-block:: yaml</p> <p>particle_regions:     - B1:         bounds: [ [ 10, 10, 5], [30, 50, 15] ]     - B2:         bounds: [ [ 15, 10, 25], [65,30,40] ]     - B3:         bounds: [ [ 30, 70, 10], [50, 90, 95] ]      - .. image:: /_static/boxes.png  :width: 400px</p> </li> </ul> </li> </ul> <p>Plane-quadrics ^^^^^^^^^^^^^^</p> <p>.. list-table:: Planes from quadrics     :widths: 50 50    :header-rows: 0</p> <ul> <li> <ul> <li> <p>.. code-block:: yaml</p> <p>particle_regions:     - P1:         quadric:           shape: { plane: [ 1, 0, 0, 0 ] }           transform: { translate: [ 20, 0, 0 ] }     - P2:         quadric:           shape: { plane: [ 0, 1, 0, 0 ] }           transform: { translate: [ 0, 20, 0 ] }     - P3:         quadric:           shape: { plane: [ 0, 0, 1, 0 ] }           transform: { translate: [ 0, 0, 20 ] }      - .. image:: /_static/planes.png  :width: 400px</p> </li> </ul> </li> </ul> <p>Cylinder-quadrics ^^^^^^^^^^^^^^^^^</p> <p>.. list-table:: Cylinders from quadrics    :widths: 50 50    :header-rows: 0</p> <ul> <li> <ul> <li> <p>.. code-block:: yaml</p> <p>particle_regions:     - P1:         quadric:           shape: cylx           transform:             scale: [ -1, 15, 15 ]             zrot: pi/4.             translate: [ 50, 50, 50 ]     - P2:         quadric:           shape: cyly           transform:             scale: [ 15, -1, 15 ]             zrot: pi/4.             translate: [ 50, 50, 50 ]     - P3:         quadric:           shape: cylz           transform:             scale: [ 15, 15, -1 ]             yrot: -pi/4.             translate: [ 50, 50, 50 ]      - .. image:: /_static/cylinders.png  :width: 400px</p> </li> </ul> </li> </ul> <p>Ellipso\u00efd-quadrics ^^^^^^^^^^^^^^^^^^</p> <p>.. list-table:: Spheres/Ellipsoids from quadrics    :widths: 50 50    :header-rows: 0</p> <ul> <li> <ul> <li> <p>.. code-block:: yaml</p> <p>particle_regions:     - S1:         quadric:           shape: sphere           transform:             scale: [ 20, 20, 20 ]             translate: [ 45, 75, 70 ]     - S2:         quadric:           shape: sphere           transform:             scale: [ 40, 30, 10 ]             translate: [ 50, 60, 20 ]     - P3:         quadric:           shape: sphere           transform:             scale: [ 50, 10, 10 ]             yrot: pi/6.             translate: [ 50, 30, 50 ]      - .. image:: /_static/spheres.png  :width: 400px</p> </li> </ul> </li> </ul> <p>Cone-quadric ^^^^^^^^^^^^</p> <p>.. list-table:: Cones from quadrics    :widths: 50 50    :header-rows: 0</p> <ul> <li> <ul> <li> <p>.. code-block:: yaml</p> <p>particle_regions:     - CO1:         quadric:           shape: conex           transform:             scale: [ 3, 0.75, 1.5 ]             translate: [ 50, 50, 50 ]     - CO2:         quadric:           shape: coney           transform:             scale: [ 1.5, 3, 0.75 ]             translate: [ 50, 50, 50 ]     - CO3:         quadric:           shape: conez           transform:             scale: [ 1, 1, 3 ]             translate: [ 50, 50, 50 ]      - .. image:: /_static/cones.png  :width: 400px</p> </li> </ul> </li> </ul> <p>Matrix4d ^^^^^^^^</p> <p>.. code-block:: yaml</p> <p>CYL9:      - quadric:          - shape: cylz          - transform:              - scale: [ 15 ang , 15 ang , 15 ang ]              - xrot: pi/4              - yrot: pi/3              - zrot: pi/6                         - translate: [ 85 ang , 85 ang , 0 ang ]      </p> <p>Range of Particles' ids ^^^^^^^^^^^^^^^^^^^^^^^</p> <p>.. code-block:: yaml</p> <p>REGID1:      - id_range: [1, 1300]</p>"},{"location":"User/DomainRegions/regions.html#user-defined-function","title":"User-defined function","text":"<p>.. code-block:: yaml</p> <p>user_function:      # WaveFrontSourceTerm      wavefront:        # first 3 values are interface plane (Pi)'s normal vector (X,Y,Z) , last one is plane offset (position of origin relative to the plane).           plane: [ -1 , 0 , 0 , 125.0 ang ]        # wave plane (normal and offset). Oriented distance to the plane, Pw(r), is used to add a sinusoid function sin(P(r))amplitude to the plane function above        wave: [ 0 , 0.1 , 0 , 0 ]        # User function is F(r) = Pi(r)+sin(Pw(r))amplitude , interface is implicit surface F(r)=0, atoms are placed everywhere where F(r)&gt;=0        amplitude: 10.0 ang    user_threshold: 0.0</p> <p>user_function:      # SphericalTemporalSourceTerm       sphere:        center: [30, 30, 30]        amplitude: 10.        radius_mean:        radius_dev:        time_mean:        time_dev:</p> <p>user_function:      # ConstantSourceTerm       constant: 10.</p>"},{"location":"User/DomainRegions/regions.html#using-the-grid-as-a-mask","title":"Using the grid as a mask","text":"<p>.. code-block:: yaml</p> <p>track_region_particles:      expr: PLANE1      name: \"PISTON\"</p>"},{"location":"User/DomainRegions/regions.html#regions-related-operations","title":"Regions-related operations","text":"<p>Tracking particles inside a region</p> <p>.. code-block:: yaml</p> <p>track_region_particles:      expr: PLANE1      name: \"PISTON\"</p> <ul> <li>A geometrical definition that can either be a parallelepiped or q quadric mathematical function that gives access to planes, spheres/ellipsoids, cones and any mathematical 3*{rd} order 3D function</li> <li>A user-defined analytical function evaluated on the 3D domain grid</li> <li>A mask read on an external file with dimensions equal to the 3D domain grid</li> <li>A range of particles ids</li> </ul> <p>Assigning regions to grid</p> <p>.. code-block:: yaml</p> <p>set_cell_values:      field_name: \"region\"      region: CYLX or CYLY or CYLZ      value: [0,1]      grid_subdiv: 10</p>"},{"location":"User/EnsemblesConstraints/index.html","title":"Simulation's graph","text":"<p>The default simulation sequence of <code>exaStamp</code> is defined in the <code>main-config.msp</code> file located in <code>exaStamp/data/config</code>. In particular, the <code>simulation</code> block allows for the entire parametrization of the MD simulation, from the initial information output and hardware initialization to the main MD loop and hardware finalization.</p> <p>Default YAML block for an exaStamp simulation</p> <pre><code>simulation:\n  name: MySimulation\n  body:\n    - print_logo_banner                                   # Print the exaStamp banner\n    - hw_device_init                                      # Default communicator + CUDA initialization\n    - make_empty_grid                                     # Create an empty grid\n    - grid_flavor                                         # Define what information is attached to the grid\n    - global                                              # Global simulation controls\n    - init_parameters                                     # Additional control parameters\n    - generate_default_species                            # Generate default species\n    - particle_regions                                    # Geometrical regions definition\n    - preinit_rcut_max                                    # Automatic cell_size calculation\n    - domain                                              # Simulation domain definition\n    - init_prolog                                         # Initialization prologue\n    - input_data                                          # Populate domain with particles\n    - species:                                            # Species definition\n        verbose: false\n        fail_if_empty: true\n    - grid_post_processing                                # Grid memory compaction\n    - reduce_species_after_read                           # Update particle species\n    - init_rcut_max                                       # Update neighborhood distance and displacement tolerance\n    - print_domain                                        # Print Domain information\n    - performance_adviser: { verbose: true }              # Print performance advices\n    - do_init_temperature                                 # Initialize temperature if needed\n    - init_epilog                                         # Initialization epilogue\n    - species:                                            # Species definition recheck\n        verbose: true\n        fail_if_empty: true\n    - first_iteration                                     # Simulation first iteration\n    - compute_loop                                        # Simulation compute loop\n    - simulation_epilog                                   # Simulation finalization\n    - hw_device_finalize                                  # CUDA finalization\n</code></pre> <p>In this YAML block, some operators are mandatory. We provide in the next section a input deck example with the minimal information required. In addition, some of the YAML operators defined in the <code>simulation</code> block are defined in other configuration files located in <code>exaStamp/data/config</code>. However, the mandatory blocks to be defined by the user to build a minimal input deck for exaStamp are:</p> <pre><code>   - global         # Global control of simulation parameters\n   - species        # Definition of the particles' species\n   - compute_force  # Choice of the interatomic potential\n   - domain         # Definition of the simulation's domain\n   - input_data     # Population of the domain with particles\n</code></pre> <p>The next section provides a basic example on how to build an <code>exaStamp</code> input deck with the minimal information required.</p> <pre><code>graph TD\n  A[\"`**Initialization**\n  Detect GPU Support\n  Create Grid\n  Preinit cutoff radius`\"]\n\n  B[Creating Particle Regions]\n\n  C[\"`**Input Data**\n  lattice, xyz, MpiIO\n  + field initialization`\"]\n\n  D[\"`**Firs Timestep**\n  Init partcles\n  - move\n  - migration\n  - neighbor lists\n  - compute forces`\"]\n\n  E[Time Loop];\n  F[test];\n  G[coucou];\n\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt; D;\n  D --&gt; E;\n  E --&gt; F;\n  F --&gt; G\n  G---&gt;|\"`t inf tfin`\"| D;\n\n</code></pre> <p>B --&gt;|Yes| C[Hmm...];   C --&gt; D[Debug];   D --&gt; B;   B ----&gt;|No| E[Yay!];   E ===&gt; newLines[\"<code>Line1                     Line 2                     Line 3</code>\"]  </p>"},{"location":"User/EnsemblesConstraints/deformation.html","title":"Deformation","text":"<p>.. _deformation_paths:</p>"},{"location":"User/EnsemblesConstraints/deformation.html#deformation-paths","title":"Deformation Paths","text":""},{"location":"User/EnsemblesConstraints/deformation.html#constant-strain-rate","title":"Constant Strain-Rate","text":""},{"location":"User/EnsemblesConstraints/deformation.html#interpolated","title":"Interpolated","text":""},{"location":"User/EnsemblesConstraints/deformation.html#interpolated-by-parts","title":"Interpolated By Parts","text":""},{"location":"User/EnsemblesConstraints/minimization.html","title":"Minimization","text":"<p>.. _energy_minimization:</p>"},{"location":"User/EnsemblesConstraints/minimization.html#energy-minimization","title":"Energy Minimization","text":""},{"location":"User/EnsemblesConstraints/minimization.html#gradient-descent","title":"Gradient Descent","text":""},{"location":"User/EnsemblesConstraints/npt_ensemble.html","title":"Npt ensemble","text":"<p>.. _npt_ensemble:</p>"},{"location":"User/EnsemblesConstraints/npt_ensemble.html#npt-ensemble","title":"NPT ensemble","text":""},{"location":"User/EnsemblesConstraints/npt_ensemble.html#nose-hoover-barostat","title":"Nos\u00e9-Hoover barostat","text":""},{"location":"User/EnsemblesConstraints/nve_ensemble.html","title":"Nve ensemble","text":"<p>.. _nve_ensemble:</p>"},{"location":"User/EnsemblesConstraints/nve_ensemble.html#nve-ensemble","title":"NVE ensemble","text":"<p>The time integration in <code>ExaStamp</code> is performed using the velocity form of the St\u00f6rmer-Verlet time integration algorithm, well-known as the <code>velocity-Verlet</code> algorithm. It is advantageous for atomistic simulations due to its simplicity, stability, and accuracy. It provides a straightforward method for integrating Newton's equations of motion, efficiently calculating both positions and velocities. This algorithm is symplectic, preserving the system's energy over long simulation times, which is crucial for accurately modeling physical systems. Additionally, its second-order accuracy in both time and energy ensures precise trajectory calculations, making it a preferred choice for molecular dynamics simulations where the conservation of physical properties and computational efficiency are essential. The <code>velocity-verlet</code> algorithm is integrated using the following scheme at each time step:</p> <ol> <li>Calculate position vector at full time-step:</li> </ol> <p>.. math::</p> <pre><code>\\mathbf{x} \\left( t + \\Delta t \\right) = \\mathbf{x} \\left( t \\right) + \\mathbf{v} \\left( t \\right) \\Delta t + \\mathbf{a} \\left(t\\right)\\frac{\\Delta t}{2}\n</code></pre> <ol> <li>Calculate velocity vector at half time-step:</li> </ol> <p>.. math::</p> <pre><code>\\mathbf{v} \\left( t + \\frac{\\Delta t}{2} \\right) = \\mathbf{v} \\left( t \\right) + \\mathbf{a} \\left( t \\right) \\frac{\\Delta t}{2}\n</code></pre> <ol> <li> <p>Compute the acceleration vector at full time-step \\( \\mathbf{a} \\left( t + \\Delta t\\right) \\) from the interatomic potential using the position at full time-step \\( \\mathbf{x} \\left( t + \\Delta t\\right) \\)</p> </li> <li> <p>Finally, calculate the velocity vector at full time-step:</p> </li> </ol> <p>.. math::</p> <pre><code>\\mathbf{v} \\left( t + \\Delta t \\right) = \\mathbf{v} \\left( t + \\frac{\\Delta t}{2} \\right) + \\frac{1}{2} \\mathbf{a} \\left( t + \\Delta t\\right) \\Delta t\n</code></pre> <p>In <code>ExaStamp</code>, the numerical scheme definition can be found in <code>exaStamp/data/config/config_numerical_scheme.msp</code> and the YAML block for the Velocity-Verlet scheme reads</p> <p>.. code-block:: yaml</p> <p>numerical_scheme: numerical_scheme_verlet</p> <p>numerical_scheme_verlet:      name: scheme      body:        - push_f_v_r: { dt_scale: 1.0  , xform_mode: INV_XFORM }        - push_f_v: { dt_scale: 0.5  , xform_mode: IDENTITY }        - check_and_update_particles        - compute_all_forces_energy        - push_f_v: { dt_scale: 0.5 , xform_mode: IDENTITY }</p> <p>The <code>exaNBody</code> code provides a generic operator for 1st order time-integration purposes. For example, the file <code>exaNBody/src/exanb/push_vec3_1st_order_xform.cpp</code> provides 3 different variants:</p> <ul> <li><code>push_v_r</code> : for updating positions from velocities</li> <li><code>push_f_v</code> : for updating velocities from forces (i.e. acceleration)</li> <li><code>push_f_r</code> : for udpdating positions from forces (i.e. acceleration)</li> </ul> <p>In addition, the <code>exaNBody</code> code also provides a generic operator for 2nd order time-integration purposes. For example, the file <code>exaNBody/src/exanb/push_vec3_2nd_order_xform.cpp</code> provides the following variant:</p> <ul> <li><code>push_f_v_r</code> : for updating positions from both velocities and forces (i.e. accelerations)</li> </ul> <p>Since in <code>ExaStamp</code> positions are expressed in a reduced frame, the argument <code>xform_mode: INV_XFORM</code> is mandatory when using any operator that updates the particles positions.</p>"},{"location":"User/EnsemblesConstraints/nvt_ensemble.html","title":"Nvt ensemble","text":"<p>.. _nose_hoover_thermostat:</p>"},{"location":"User/EnsemblesConstraints/nvt_ensemble.html#nose-hoover-thermostat","title":"Nos\u00e9-Hoover thermostat","text":"<p>Apply a Nos\u00e9-Hoover thermostat to the system. This thermostat actually performs the time integration of the particles so the numerical scheme YAML block is replaced in exaStamp by the one provided below. We first describe the general process of the Nos\u00e9-Hoover thermostat.</p> <ol> <li> <p>Initialization at the beginning of the simulation:</p> </li> <li> <p>Set thermostat variables and derivatives to 0:</p> </li> </ol> <p>.. math::</p> <pre><code>  \\eta = \\dot{\\eta} = \\ddot{\\eta} = 0.0\n</code></pre> <ul> <li>Set thermostat mass to 0:</li> </ul> <p>.. math::</p> <pre><code>  \\eta_M = 0.0\n</code></pre> <ul> <li>Set the thermostat coupling frequency:</li> </ul> <p>.. math::</p> <pre><code>  t_{freq} = \\frac{1}{t_{period}}\n</code></pre> <p>where \\(t_{period}\\) is the coupling period between the system and the thermostat, provided by the user.</p> <ol> <li> <p>Setup the thermostat parameters at the beginning of the simulation:</p> </li> <li> <p>Compute the current target temperature \\(T^*\\) and the corresponding degrees of freedom and target kinetic energy:</p> </li> </ol> <p>.. math::</p> <pre><code> N_{dof} = 3 N_{atoms} - 3\n</code></pre> <p>.. math::</p> <pre><code> KE^* = N_{dof} k_B T^*\n</code></pre> <p>where the \\(^*\\) upperscript denotes the target value of either kinetic energy or temperature. Note that the target temperature can evolved with time depending if the user has provided a single temperature (constant target temperature), two temperatures (linear ramp) or a list of times and temperatures greater than 2, meaning that an interpolation by parts is done to compute the target temperature at each time-step.</p> <ul> <li>Initialize masses and initial forces on thermostat variables:</li> </ul> <p>.. math::</p> <pre><code>  \\eta_M = \\frac{N_{dof} \\cdot k_B \\cdot T^*}{t_f^2} = \\frac{KE^*}{t_f^2}\n</code></pre> <ol> <li>During each time-step, supposing that the positions, velocities and forces are up-to-date, the folowing steps are performed:</li> </ol> <p>a. Compute target temperature \\(T^\\) and kinetic energy \\(KE^\\)    b. Compute current temperature \\(T_{cur}\\) and kinetic energy \\(KE_{cur}\\)        c. Compute thermostat mass :</p> <pre><code>  .. math::\n\n     \\eta_M = \\frac{N_{dof} \\cdot k_B \\cdot T^*}{t_f^2} = \\frac{KE^*}{t_f^2}\n</code></pre> <p>d. Compute acceleration and velocity of thermostat variable as well as scaling factors for atoms velocities:</p> <pre><code>  .. math::\n\n     \\ddot{\\eta} = t_f^2 \\left( \\frac{KE_{cur}}{KE^*} - 1 \\right)\n\n  .. math::\n\n     \\gamma_e = e^{\\left( -\\right)}\n</code></pre> <p>e. Perform one Nos\u00e9-Hoover integration step    f. Perform velocity update with half a </p>"},{"location":"User/EnsemblesConstraints/pistons.html","title":"Pistons","text":"<p>.. _external_forces:</p>"},{"location":"User/EnsemblesConstraints/pistons.html#pistons","title":"Pistons","text":""},{"location":"User/EnsemblesConstraints/pistons.html#planar-wall","title":"Planar Wall","text":""},{"location":"User/EnsemblesConstraints/pistons.html#circular-wall","title":"Circular Wall","text":""},{"location":"User/EnsemblesConstraints/pistons.html#spherical-wall","title":"Spherical Wall","text":""},{"location":"User/EnsemblesConstraints/Thermostats/index.html","title":"Thermostats","text":""},{"location":"User/EnsemblesConstraints/Thermostats/berendsen.html","title":"Berendsen","text":""},{"location":"User/EnsemblesConstraints/Thermostats/berendsen.html#berendsen-thermostat","title":"Berendsen Thermostat","text":"<p>Apply a Berendsen thermostat to the system by rescaling the atoms velocities at each timestep. This thermostatting method is a weak coupling between the system and a heat bath with the target temperature. Kinetic energy fluctuations are suppressed with the Berendsen thermostat which cannot produce trajectories consistent with the canonical ensemble. Atoms velocities are rescaled at each timestep such that the rate of change exponentially decays with some characteristic time \\(\\tau\\). This thermostat has to b</p> <p>.. math::</p> <p>\\frac{dT}{dt} = \\frac{1}{\\tau} \\left( T^* - T \\right)</p> <p>with \\(T^*\\) the target temperature and  \\(T\\) the current system's temperature. The increase in temperature between two time steps reads</p> <p>.. math::</p> <p>\\Delta T = \\frac{dt}{\\tau} \\left( T^* - T \\right)</p> <p>leading to the following scaling factor of atoms velocities:</p> <p>.. math::</p> <p>\\lambda = \\sqrt{1 + \\frac{dt}{\\tau} \\left( \\frac{T^*}{T} - 1\\right)}</p> <p>.. warning::</p> <p>This thermostat has to be appended to the end of the numerical scheme <code>numerical_scheme</code> YAML block since no time integration is performed by this operator and it needs the instantaneous temperature, contrarily to the Nos\u00e9-Hoover thermostat (See :ref:<code>nose_hoover_thermostat</code>). In addition, prior to this operator, the <code>thermodynamic_state</code> operator has to be called since the instantaneous temperature is required by the thermostat.</p> <p>The Berendsen thermostat can be defined in the input file using three ways that are presented in the following <code>YAML</code> block:</p> <p>.. code-block:: yaml    :caption: Different ways of defining a Berendsen thermostat</p> <p># 1st solution: constant target temperature    berendsen_thermostat:      T: 300. K      tau: 0.1 ps</p> <p># 2nd solution: linear target temperature       berendsen_thermostat:      Tstart: 5. K      Tstop: 1000. K      tau: 0.1 ps</p> <p># 3rd solution: linearly interpolated target temperature    berendsen_thermostat:      tserie: [0, 10., 20.]      Tserie: [5., 500., 500.]      tau: 0.1 ps</p> <p>Finally, since the Berendsen thermostat directly operates on atomic forces and needs the instantaneous temperature to be calculated by the <code>thermodynamic_state</code> operator, both must be added to the <code>numerical_scheme</code> YAML block as follows:</p> <p>.. code-block:: yaml</p> <p>thermostat: berendsen_thermostat</p> <p>compute_force:      - interatomic_force_operator_1</p> <p>numerical_scheme:      verlet_first_half      check_and_update_particles      compute_all_forces_energy      verlet_second_half      simulation_thermodynamic_state      thermostat</p> <p>Below are displayed the different parameters of <code>langevin_thermostat</code> as well as their types and corresponding examples.</p> <p>.. list-table:: Properties for the Berendsen thermostat    :widths: 40 40 40 40    :header-rows: 1</p> <ul> <li> <ul> <li>Property</li> <li>Denomination</li> <li>Data Type</li> <li>Example</li> </ul> </li> <li> <ul> <li><code>T</code></li> <li>target temperature (K)</li> <li>float</li> <li> <p>.. code-block:: yaml</p> <p>T: 300 K    * - <code>Tstart</code>      - starting target temperature (K)      - float      - .. code-block:: yaml</p> <p>Tstart: 300 K    * - <code>Tstop</code>      - final target temperature (K)      - float      - .. code-block:: yaml</p> <p>Tstop: 1000 K    * - <code>Tserie</code>      - serie of target temperature (K)      - float      - .. code-block:: yaml</p> <p>Tserie: [5, 200, 1000, 100]    * - <code>tserie</code>      - serie of physical times (ps)      - float      - .. code-block:: yaml</p> <p>tserie: [0,10,20,30]    * - <code>tau</code>      - coupling characteristic time (ps)      - float      - .. code-block:: yaml</p> <p>tau: 0.1 ps</p> </li> </ul> </li> </ul> <p>.. warning::</p> <p>When using a Berendsen thermostat, the target temperature must be defined by one of the three ways presented above. If it is misdefined, the simulation will be aborted.</p>"},{"location":"User/EnsemblesConstraints/Thermostats/langevin.html","title":"Langevin","text":""},{"location":"User/EnsemblesConstraints/Thermostats/langevin.html#langevin-thermostat","title":"Langevin Thermostat","text":"<p>Apply a Langevin thermostat to the system to model an interaction with a background implicit solvent. Using a Langevin thermostat, the total force on each atom of the system reads:</p> <p>.. math::</p> <p>F = F_c + F_f + F_r</p> <p>where \\(F_c\\) is the classical force computed via the interatomic potential and \\(F_f\\) corresponds to the first term added by the Langevin thermostat. This first therm is a viscous damping therm directly proportional to the particle's velocity:</p> <p>.. math::</p> <p>F_f = - \\frac{m}{\\gamma} v</p> <p>with the prefacto defined as the ratio between the particle's mass and the damping parameter \\(\\gamma\\) defined by the user. The second term added by the Langevin thermostat mimics a force due to solvent atoms at a temperature \\(T\\) randomly bumping the particle. From the fluctuation/dissipation theorem, the magnitude of this term is</p> <p>.. math::</p> <p>F_r \\propto \\sqrt{\\frac{m k_b T}{dt \\gamma}}</p> <p>with \\(m\\) the mass of the particle, \\(k_b\\) the Boltzman constant, \\(T\\) the target temperature, \\(dt\\) the timestep and \\(\\gamma\\) the damping parameter. The proportionality of this term is ensured by random numbers generation using a uniform distribution.</p> <p>.. warning::</p> <p>This thermostat has to be appended to the <code>compute_force</code> YAML block since no time integration is performed by this operator, contrarily to the Nos\u00e9-Hoover thermostat (See :ref:<code>nose_hoover_thermostat</code>).</p> <p>The Langevin thermostat can be defined in the input file using three ways that are presented in the following <code>YAML</code> block:</p> <p>.. code-block:: yaml    :caption: Different ways of defining a Langevin thermostat</p> <p># 1st solution: constant target temperature    langevin_thermostat:      T: 300. K      gamma: 0.1 ps^-1</p> <p># 2nd solution: linear target temperature       langevin_thermostat:      Tstart: 5. K      Tstop: 1000. K      gamma: 0.1 ps^-1</p> <p># 3rd solution: linearly interpolated target temperature    langevin_thermostat:       tserie: [0, 10., 20.]       Tserie: [5., 500., 500.]       gamma: 0.1 ps^-1</p> <p>Finally, since the Langevin thermostat directly operates on atomic forces, it can be added to the <code>compute_force</code> YAML block as follows:</p> <p>.. code-block:: yaml    :caption: Extending the force operator with a Langevin thermostat</p> <p>compute_force:      - interatomic_force_operator_1      - langevin_thermostat</p> <p>Below are displayed the different parameters of <code>langevin_thermostat</code> as well as their types and corresponding examples.</p> <p>.. list-table:: Properties for the Langevin thermostat    :widths: 40 40 40 40    :header-rows: 1</p> <ul> <li> <ul> <li>Property</li> <li>Denomination</li> <li>Data Type</li> <li>Example</li> </ul> </li> <li> <ul> <li><code>T</code></li> <li>target temperature (K)</li> <li>float</li> <li> <p>.. code-block:: yaml</p> <p>T: 300 K    * - <code>Tstart</code>      - starting target temperature (K)      - float      - .. code-block:: yaml</p> <p>Tstart: 300 K    * - <code>Tstop</code>      - final target temperature (K)      - float      - .. code-block:: yaml</p> <p>Tstop: 1000 K    * - <code>Tserie</code>      - serie of target temperature (K)      - float      - .. code-block:: yaml</p> <p>Tserie: [5, 200, 1000, 100]    * - <code>tserie</code>      - serie of physical times (ps)      - float      - .. code-block:: yaml</p> <p>tserie: [0,10,20,30]    * - <code>gamma</code>      - damping constant (ps^-1)      - float      - .. code-block:: yaml</p> <p>gamma: 0.1 ps^-1</p> </li> </ul> </li> </ul> <p>.. warning::</p> <p>When using a Langevin thermostat, the target temperature must be defined by one of the three ways presented above. If it is misdefined, the simulation will be aborted.</p>"},{"location":"User/ForceFields/index.html","title":"Force Fields","text":"<p>In the present section, we explain in detail the different interatomic potentials and force fields that can be used in exaStamp. Below are links (you can also access them using the left table of contents) to all types of interatomic potentials available in the code:</p> <ul> <li>Defining an interatomic potential</li> <li>Pair Potentials</li> <li>Embedded Atom Model (EAM)</li> <li>Modified EAM</li> <li>Reactive potentials</li> <li>Machine Learning Interatomic potentials (MLIP)</li> <li>Electrostatic potentials</li> <li>Intramolecular potentials</li> </ul>"},{"location":"User/ForceFields/general.html","title":"Defining the interatomic potential","text":"<p>In exaStamp, the interatomic potential or force field to be user during the simulation is simply defined by the <code>compute_force</code> operator. This operator is by definition a list of operators associated to a single or multiple instances of interatomic potentials. See a generic usage example below.</p> <p>Example</p> <pre><code># Single potential\ncompute_force: potential_1\ncompute_force:\n  - potential_1\n\n# Multiple potentials\ncompute_force: [ potential_1, potential_2, potential_1 ]\ncompute_force:\n  - potential_1\n  - potential_2\n  - potential_3\n</code></pre> <p>where <code>potential_1</code>, <code>potential_2</code> and <code>potential_3</code> can be different interatomic potentials or any operator that affects the atomic forces such as a Langevin thermostat for example.</p> <p>Below are links (you can also access them using the left table of contents) to all types of interatomic potentials available in the code:</p> <ul> <li>Pair Potentials</li> <li>Embedded Atom Model (EAM)</li> <li>Modified EAM</li> <li>Reactive potentials</li> <li>Machine Learning Interatomic potentials (MLIP)</li> <li>Electrostatic potentials</li> <li>Intramolecular potentials</li> </ul>"},{"location":"User/ForceFields/EAM/index.html","title":"Embedded-Atom-Model","text":""},{"location":"User/ForceFields/EAM/alloy.html","title":"EAM alloy","text":""},{"location":"User/ForceFields/EAM/johnson.html","title":"EAM Johnson","text":""},{"location":"User/ForceFields/EAM/ravelo.html","title":"EAM Ravelo","text":""},{"location":"User/ForceFields/EAM/sutton_chen.html","title":"EAM Sutton-Chen","text":""},{"location":"User/ForceFields/EAM/tab.html","title":"EAM tabulated","text":""},{"location":"User/ForceFields/EAM/vniitf.html","title":"EAM vniitf","text":""},{"location":"User/ForceFields/Electrostatics/index.html","title":"Electrostatic Potentials","text":""},{"location":"User/ForceFields/Electrostatics/long.html","title":"Long range","text":""},{"location":"User/ForceFields/Electrostatics/long.html#ewald-summation","title":"Ewald Summation","text":""},{"location":"User/ForceFields/Electrostatics/long.html#particle-particle-particle-mesh","title":"Particle-Particle Particle-Mesh","text":""},{"location":"User/ForceFields/Electrostatics/short.html","title":"Short range","text":""},{"location":"User/ForceFields/Electrostatics/short.html#standard-coulombic-interaction","title":"Standard Coulombic Interaction","text":""},{"location":"User/ForceFields/Electrostatics/short.html#damped-shifted-force","title":"Damped Shifted Force","text":""},{"location":"User/ForceFields/Electrostatics/short.html#wolf-summation","title":"Wolf Summation","text":""},{"location":"User/ForceFields/Electrostatics/short.html#reaction-field","title":"Reaction Field","text":""},{"location":"User/ForceFields/Intramolecular/index.html","title":"Intramolecular interactions","text":"<ul> <li>Bond interaction</li> <li>Torsion interaction</li> <li>Dihedral interaction</li> <li>Improper interaction</li> </ul>"},{"location":"User/ForceFields/Intramolecular/bonds.html","title":"Bond interactions","text":""},{"location":"User/ForceFields/Intramolecular/dihedrals.html","title":"Dihedral interactions","text":""},{"location":"User/ForceFields/Intramolecular/impropers.html","title":"Improper interactions","text":""},{"location":"User/ForceFields/Intramolecular/torsions.html","title":"Torsion interactions","text":""},{"location":"User/ForceFields/MEAM/index.html","title":"Modified-Embedded Atom Model","text":""},{"location":"User/ForceFields/MEAM/meam.html","title":"MEAM","text":""},{"location":"User/ForceFields/MEAM/meam_lj.html","title":"MEAM + LJ","text":""},{"location":"User/ForceFields/MLIP/index.html","title":"Machine Learning Interatomic Potentials","text":""},{"location":"User/ForceFields/MLIP/ace.html","title":"ACE - Atomic Cluster Expansion","text":"<p>Under construction</p>"},{"location":"User/ForceFields/MLIP/nnp.html","title":"NNP - Neural Network Potential","text":"<p>Under construction</p>"},{"location":"User/ForceFields/MLIP/snap.html","title":"SNAP - Spectral Neighbor Analysis Potential","text":"<p>Under construction</p>"},{"location":"User/ForceFields/Pair/index.html","title":"Pair Potentials","text":"<p>The present section contains all pair potentials available in exaStamp. Computation strategies (non symmetric, symmetric, single specy, multiple species and hybrid potentials) are first presented. Then, each pair potential formulation is explained in detail with usage examples.</p> <ul> <li>Strategies</li> <li> <p>Models</p> <ul> <li>Buckingham</li> <li>Coul cut</li> <li>Coul wolf</li> <li>Exp-6</li> <li>Exp-6 + Reaction Field</li> <li>Lennard-Jones</li> <li>Lennard-Jones + exp6 + RF</li> <li>Lennard-Jones + exp6 + RF + LJ</li> <li>Lennard-Jones + RF</li> <li>Lennard-Jones + Wolf</li> <li>Reaction Field</li> <li>Relax</li> <li>Tabulated pair</li> <li>ZBL</li> <li>Zero</li> </ul> </li> </ul>"},{"location":"User/ForceFields/Pair/Models/index.html","title":"Available Models","text":"<p>The present section contains all pair potentials available in exaStamp. Computation strategies (non symmetric, symmetric, single specy, multiple species and hybrid potentials) are first presented. Then, each pair potential formulation is explained in detail with usage examples.</p> <ul> <li>Buckingham</li> <li>Coul cut</li> <li>Coul wolf</li> <li>Exp-6</li> <li>Exp-6 + Reaction Field</li> <li>Lennard-Jones</li> <li>Lennard-Jones + exp6 + RF</li> <li>Lennard-Jones + exp6 + RF + LJ</li> <li>Lennard-Jones + RF</li> <li>Lennard-Jones + Wolf</li> <li>Reaction Field</li> <li>Relax</li> <li>Tabulated pair</li> <li>ZBL</li> <li>Zero</li> </ul>"},{"location":"User/ForceFields/Pair/Models/buckingham.html","title":"Buckingham","text":""},{"location":"User/ForceFields/Pair/Models/buckingham.html#description","title":"Description","text":"<p>The <code>buckingham_compute_force</code> operator calculates the Buckingham (exp-6) pair potential given by</p> \\[ E(r) = A \\, e^{-r/\\rho} - \\frac{C}{r^{6}} \\quad \\text{for} \\quad r&lt;r_c \\] <p>with \\(r_c\\) the cutoff and \\((A,\\rho,C)\\) the potential parameters described in the following table.</p> Parameter Units Description \\(A\\) energy Short-range repulsive amplitude \\(\\rho\\) distance Repulsive decay length \\(C\\) energy\u00b7distance\\(^6\\) Dispersion (attractive) coefficient \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/buckingham.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>buckingham_compute_force:\n  rcut: VALUE UNITS\n  parameters: { A: VALUE UNITS , rho: VALUE UNITS , C: VALUE UNITS }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/buckingham.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant\nbuckingham_compute_force:\n  parameters: { A: 1000.0 eV , rho: 0.30 ang , C: 1200.0 eV*ang^6 }\n  rcut: 8.0 ang\n\n# Symetric variant\nbuckingham_compute_force_symetric:\n  parameters: { A: 1000.0 eV , rho: 0.30 ang , C: 1200.0 eV*ang^6 }\n  rcut: 8.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>buckingham_multi_force:\n  rcut: 8.0 ang\n  common_parameters: { A: 0.0 , rho: 0.0 , C: 0.0 }\n  parameters:\n    - { type_a: O , type_b: O , rcut: 8.0 ang , parameters: { A: 9547.96 eV , rho: 0.21916 ang , C: 32.0 eV*ang^6 } }\n    - { type_a: Si , type_b: O , rcut: 8.0 ang , parameters: { A: 18003.7572 eV , rho: 0.2052 ang , C: 133.5381 eV*ang^6 } }\n</code></pre>"},{"location":"User/ForceFields/Pair/Models/coul_cut.html","title":"Coulomb (cutoff)","text":""},{"location":"User/ForceFields/Pair/Models/coul_cut.html#description","title":"Description","text":"<p>The <code>coul_cut_compute_force</code> operator calculates the Coulomb pair potential with a simple real-space cutoff:</p> \\[ E(r) = \\frac{1}{4\\pi \\varepsilon_0 \\varepsilon_r} \\frac{q_i q_j}{r} \\quad \\text{for} \\quad r&lt;r_c \\quad \\text{(and } E=0 \\text{ for } r\\ge r_c\\text{)} \\] <p>with \\(r_c\\) the cutoff and \\(\\varepsilon_r\\) the relative permittivity of the medium.</p> Parameter Units Description \\(\\varepsilon_r\\) \u2014 Relative dielectric constant of medium \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/coul_cut.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>coul_cut_compute_force:\n  rcut: VALUE UNITS\n  parameters: { epsilon_r: VALUE }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/coul_cut.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant\ncoul_cut_compute_force:\n  parameters: { epsilon_r: 1.0 }\n  rcut: 12.0 ang\n\n# Symetric variant\ncoul_cut_compute_force_symetric:\n  parameters: { epsilon_r: 1.0 }\n  rcut: 12.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>coul_cut_multi_force:\n  rcut: 12.0 ang\n  common_parameters: { epsilon_r: 1.0 }\n  parameters:\n    - { type_a: Na , type_b: Cl , rcut: 12.0 ang , parameters: { } }\n    - { type_a: Na , type_b: Na , rcut: 12.0 ang , parameters: { } }\n</code></pre>"},{"location":"User/ForceFields/Pair/Models/coul_wolf_pair.html","title":"Coulomb (Wolf)","text":""},{"location":"User/ForceFields/Pair/Models/coul_wolf_pair.html#description","title":"Description","text":"<p>The <code>coul_wolf_compute_force</code> operator calculates the damped, truncated Coulomb (Wolf) pair potential:</p> \\[ E(r) = \\frac{1}{4\\pi \\varepsilon_0 \\varepsilon_r} \\, q_i q_j \\left[\\frac{\\operatorname{erfc}(\\alpha r)}{r} - \\frac{\\operatorname{erfc}(\\alpha r_c)}{r_c}\\right] \\quad \\text{for} \\quad r&lt;r_c \\] <p>with damping parameter \\(\\alpha\\) and cutoff \\(r_c\\). The constant term ensures \\(E(r_c)=0\\).</p> Parameter Units Description \\(\\alpha\\) 1/distance Wolf damping parameter \\(\\varepsilon_r\\) \u2014 Relative dielectric constant of medium \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/coul_wolf_pair.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>coul_wolf_compute_force:\n  rcut: VALUE UNITS\n  parameters: { alpha: VALUE UNITS^-1 , epsilon_r: VALUE }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/coul_wolf_pair.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant\ncoul_wolf_compute_force:\n  parameters: { alpha: 0.20 ang^-1 , epsilon_r: 1.0 }\n  rcut: 10.0 ang\n\n# Symetric variant\ncoul_wolf_compute_force_symetric:\n  parameters: { alpha: 0.20 ang^-1 , epsilon_r: 1.0 }\n  rcut: 10.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>coul_wolf_multi_force:\n  rcut: 10.0 ang\n  common_parameters: { alpha: 0.20 ang^-1 , epsilon_r: 1.0 }\n  parameters:\n    - { type_a: Na , type_b: Cl , rcut: 10.0 ang , parameters: { } }\n    - { type_a: Na , type_b: Na , rcut: 10.0 ang , parameters: { } }\n</code></pre>"},{"location":"User/ForceFields/Pair/Models/exp6.html","title":"Exponential-6","text":""},{"location":"User/ForceFields/Pair/Models/exp6.html#description","title":"Description","text":"<p>The <code>exp6_compute_force</code> operator calculates the normalized exponential-6 pair potential parameterized by well depth, equilibrium distance, and hardness:</p> \\[ E(r) = \\varepsilon \\left[ \\frac{6}{\\alpha-6}\\,\\exp\\!\\big(\\alpha(1 - r/r_m)\\big) - \\frac{\\alpha}{\\alpha-6}\\left(\\frac{r_m}{r}\\right)^{6} \\right] \\quad \\text{for} \\quad r&lt;r_c \\] <p>with \\(r_c\\) the cutoff and \\((\\varepsilon,r_m,\\alpha)\\) as defined below.</p> Parameter Units Description \\(\\varepsilon\\) energy Well depth \\(r_m\\) distance Distance at potential minimum \\(\\alpha\\) \u2014 Repulsion steepness (dimensionless) \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/exp6.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>exp6_compute_force:\n  rcut: VALUE UNITS\n  parameters: { epsilon: VALUE UNITS , r_m: VALUE UNITS , alpha: VALUE }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/exp6.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant\nexp6_compute_force:\n  parameters: { epsilon: 0.0100 eV , r_m: 3.80 ang , alpha: 13.0 }\n  rcut: 8.0 ang\n\n# Symetric variant\nexp6_compute_force_symetric:\n  parameters: { epsilon: 0.0100 eV , r_m: 3.80 ang , alpha: 13.0 }\n  rcut: 8.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>exp6_multi_force:\n  rcut: 8.0 ang\n  common_parameters: { epsilon: 0.0 , r_m: 0.0 , alpha: 0.0 }\n  parameters:\n    - { type_a: Ar , type_b: Ar , rcut: 8.0 ang , parameters: { epsilon: 0.0100 eV , r_m: 3.80 ang , alpha: 13.0 } }\n    - { type_a: Kr , type_b: Ar , rcut: 8.0 ang , parameters: { epsilon: 0.0120 eV , r_m: 4.00 ang , alpha: 12.5 } }\n</code></pre>"},{"location":"User/ForceFields/Pair/Models/lj.html","title":"Lennard-Jones","text":""},{"location":"User/ForceFields/Pair/Models/lj.html#description","title":"Description","text":"<p>The <code>lj_compute_force</code> operator calculates the standard 12/6 Lennard-Jones pair potential computes the given by</p> \\[ E = 4 \\epsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right] \\quad \\text{for} \\quad r&lt;r_c \\] <p>with \\(r_c\\), \\(\\epsilon\\) and \\(\\sigma\\) the potential parameters decribed in the following table.</p> Parameter Units Description \\(\\varepsilon\\) energy Depth of energy well \\(\\sigma\\) distance Distance of energy well from central particle \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/lj.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>lj_compute_force:\n  rcut: VALUE UNITS\n  parameters: { epsilon: VALUE UNITS , sigma: VALUE UNITS }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/lj.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant\nlj_compute_force:\n  parameters: { epsilon: 0.0104 eV , sigma: 3.4 ang }\n  rcut: 8.0 ang\n\n# Symetric variant\nlj_compute_force_symetric:\n  parameters: { epsilon: 0.0104 eV , sigma: 3.4 ang }\n  rcut: 8.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>lj_multi_force:\n  rcut: 8. ang\n  common_parameters: { epsilon: 0.0 , sigma: 0.0 }\n  parameters:\n    - { type_a: Zn , type_b: Zn , rcut: 6.10 ang , parameters: { epsilon: 2.522E-20 J , sigma: 0.244E-09 m } }\n    - { type_a: Cu , type_b: Zn , rcut: 5.89 ang , parameters: { epsilon: 4.853E-20 J , sigma: 0.236E-09 m } }\n</code></pre>"},{"location":"User/ForceFields/Pair/Models/reaction_field.html","title":"Reaction Field","text":""},{"location":"User/ForceFields/Pair/Models/reaction_field.html#description","title":"Description","text":"<p>The <code>reaction_field_compute_force</code> operator calculates the reaction-field (RF) electrostatic pair potential assuming a dielectric continuum beyond the cutoff:</p> <p>For \\(r \\le r_c\\), $$ E(r) = \\frac{1}{4\\pi \\varepsilon_0}\\, q_i q_j \\left[ \\frac{1}{r} + k_{\\mathrm{rf}} r^2 - C_{\\mathrm{rf}} \\right], $$ with $$ k_{\\mathrm{rf}} = \\frac{\\varepsilon_{\\mathrm{rf}} - 1}{2\\varepsilon_{\\mathrm{rf}} + 1}\\,\\frac{1}{r_c^3}, \\qquad C_{\\mathrm{rf}} = \\frac{1}{r_c} + k_{\\mathrm{rf}} r_c^2, $$ so that \\(E(r_c)=0\\). For \\(r&gt;r_c\\), \\(E=0\\).</p> Parameter Units Description \\(\\varepsilon_{\\mathrm{rf}}\\) \u2014 Dielectric constant of the surrounding continuum \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/reaction_field.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>reaction_field_compute_force:\n  rcut: VALUE UNITS\n  parameters: { epsilon_rf: VALUE }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/reaction_field.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant\nreaction_field_compute_force:\n  parameters: { epsilon_rf: 78.5 }  # e.g., water at room T\n  rcut: 12.0 ang\n\n# Symetric variant\nreaction_field_compute_force_symetric:\n  parameters: { epsilon_rf: 78.5 }\n  rcut: 12.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>reaction_field_multi_force:\n  rcut: 12.0 ang\n  common_parameters: { epsilon_rf: 78.5 }\n  parameters:\n    - { type_a: Na , type_b: Cl , rcut: 12.0 ang , parameters: { } }\n    - { type_a: Na , type_b: Na , rcut: 12.0 ang , parameters: { } }\n</code></pre>"},{"location":"User/ForceFields/Pair/Models/zbl.html","title":"Ziegler\u2013Biersack\u2013Littmark (ZBL)","text":""},{"location":"User/ForceFields/Pair/Models/zbl.html#description","title":"Description","text":"<p>The <code>zbl_compute_force</code> operator calculates the screened nuclear repulsion using the universal ZBL potential:</p> \\[ E(r) = \\frac{Z_1 Z_2 e^2}{4\\pi \\varepsilon_0} \\, \\frac{\\Phi(r/a)}{r} \\quad \\text{for} \\quad r&lt;r_c, \\] <p>where the screening function \\(\\Phi(x)\\) is</p> \\[ \\Phi(x)= 0.1818\\,e^{-3.2x} + 0.5099\\,e^{-0.9423x} + 0.2802\\,e^{-0.4029x} + 0.02817\\,e^{-0.2016x}, \\] <p>and the screening length \\(a\\) is</p> \\[ a = 0.8854\\,a_0\\,\\big(Z_1^{0.23} + Z_2^{0.23}\\big)^{-1}, \\] <p>with \\(a_0\\) the Bohr radius.</p> \\[ E_{ij} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Z_i Z_j e^2}{r_{ij}} \\phi \\left(r_{ij}/a \\right) + S\\left(r_{ij}\\right) \\] <p>where \\(e\\) is the electron charge density, \\(\\\\epsilon_0\\) the vacuum electrical permittivity, \\(Z_i\\) and \\(Z_j\\) the nuclear charges of the two interacting atoms. The scaling factor \\(a\\) is defined as:</p> \\[ a = \\frac{0.46850}{Z_i^{0.23}+Z_j^{0.23}} \\] <p>and the switch function reads</p> \\[   \\phi\\left(x\\right) = 0.18175e^{-3.19980x}+0.50986e^{-0.94229x}+0.28022e^{-0.40290x}+0.02817e^{-0.20162x} \\] Parameter Units Description \\(Z_1\\) \u2014 Atomic number of species A \\(Z_2\\) \u2014 Atomic number of species B \\(r_c\\) distance Cutoff radius"},{"location":"User/ForceFields/Pair/Models/zbl.html#yaml-syntax","title":"YAML syntax","text":"<pre><code>zbl_compute_force:\n  rcut: VALUE UNITS\n  parameters: { Z1: VALUE , Z2: VALUE }\n</code></pre> <ul> <li> VALUE = Physical value of the intended parameter.</li> <li> UNITS = Units of the provided value that will be passed to the conversion helper for internal units conversion.</li> </ul>"},{"location":"User/ForceFields/Pair/Models/zbl.html#usage-examples","title":"Usage examples","text":"<p>Systems with a single atomic specy</p> <pre><code># Default variant (Al\u2013Al)\nzbl_compute_force:\n  parameters: { Z1: 13 , Z2: 13 }\n  rcut: 2.0 ang\n\n# Symetric variant\nzbl_compute_force_symetric:\n  parameters: { Z1: 13 , Z2: 13 }\n  rcut: 2.0 ang  \n</code></pre> <p>Systems with multiple atomic species</p> <pre><code>zbl_multi_force:\n  rcut: 2.0 ang\n  common_parameters: { Z1: 0 , Z2: 0 }\n  parameters:\n    - { type_a: Al , type_b: Al , rcut: 2.0 ang , parameters: { Z1: 13 , Z2: 13 } }\n    - { type_a: Al , type_b: Cu , rcut: 2.0 ang , parameters: { Z1: 13 , Z2: 29 } }\n</code></pre> <p># Ziegler-Biersack-Littmark</p> <p>The Ziegler-Biersack-Littmark pair potential computes the screened nuclear repulstion that describes high-enery collisions between atoms. Energy, forces and curvature are smoothly rapmed to zero between and inner and an outer cutoff thanks to a switch function. The energy contribution for to atoms \\(i\\) and \\(j\\) at a idstance \\(r_{ij}\\) from each other reads</p> <p>.. math::</p> <p>E_{ij} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Z_i Z_j e^2}{r_{ij}} \\phi \\left(r_{ij}/a \\right) + S\\left(r_{ij}\\right)</p> <p>where \\(e\\) is the electron charge density, \\(\\\\epsilon_0\\) the vacuum electrical permittivity, \\(Z_i\\) and \\(Z_j\\) the nuclear charges of the two interacting atoms. The scaling factor \\(a\\) is defined as:</p> <p>.. math::</p> <p>a = \\frac{0.46850}{Z_i^{0.23}+Z_j^{0.23}}</p> <p>and the switch function reads</p> <p>.. math::</p> <p>\\phi\\left(x\\right) = 0.18175e^{-3.19980x}+0.50986e^{-0.94229x}+0.28022e^{-0.40290x}+0.02817e^{-0.20162x}</p> <p>.. list-table:: Ziegler-Biersack-Littmark Parameters    :widths: 40 40    :header-rows: 1    :align: center</p> <ul> <li> <ul> <li>Denomination</li> <li>Units</li> </ul> </li> <li> <ul> <li>\\(Z_i\\)</li> <li>atomic charge</li> </ul> </li> <li> <ul> <li>\\(Z_j\\)</li> <li>atomic charge</li> </ul> </li> <li> <ul> <li>\\(\\r_1\\)</li> <li>distance</li> </ul> </li> <li> <ul> <li>\\(r_c\\)</li> <li>distance</li> </ul> </li> </ul> <p>Below is a usage example for a system containing one single atomic specy</p> <p>.. code-block:: yaml</p> <p># Basic force computation    zbl_compute_force: &amp;zbl_params      parameters: { r1: 5. ang , rc: 5.68 ang }      rcut: 5.68 ang</p> <p># Symetric interaction variant    zbl_compute_force_symetric: *zbl_params</p> <p># General force calculation block called in the integration scheme    compute_force:      - zbl_compute_force</p> <p>The atomic charges are not passed as parameters of the potential since they are already attached to particle species and defined in the <code>YAML</code> block <code>species</code> defined in the :ref:<code>particles-species</code> section. In the case of a system with multiple species, the force operator can be defined as follows</p> <p>.. code-block:: yaml</p> <p># Basic force computation    compute_force_pair_multimat:      potentials:        - { type_a: Si , type_b: Si , potential: zbl , rcut: 5.68 ang , parameters: { r1: 5 ang, rc: 5.68 ang } }        - { type_a: Si , type_b: O  , potential: zbl , rcut: 5.4 ang , parameters: { r1: 5.1 ang, rc: 5.4 ang } }</p>"},{"location":"User/ForceFields/Pair/Models/zero.html","title":"Zero","text":"<p>The Zero pair potential zeros out the energy and forces contribution between two atoms.</p> <p>.. list-table:: Lennard-Jones Parameters    :widths: 40 40    :header-rows: 1    :align: center</p> <ul> <li> <ul> <li>Denomination</li> <li>Units</li> </ul> </li> <li> <ul> <li>\\(\\\\epsilon\\)</li> <li>energy</li> </ul> </li> <li> <ul> <li>\\(\\\\sigma\\)</li> <li>distance</li> </ul> </li> <li> <ul> <li>\\(r_c\\)</li> <li>distance</li> </ul> </li> </ul> <p>Below is a usage example for a system containing one single atomic specy</p> <p>.. code-block:: yaml</p> <p># Basic force computation    zero_compute_force:      rcut: 7.0 ang</p> <p># General force calculation block called in the integration scheme    compute_force:      - zero_compute_force</p> <p>In the case of a system with multiple species, the force operator can be defined as follows</p> <p>.. code-block:: yaml</p> <p># Basic force computation    compute_force_pair_multimat:      potentials:        - { type_a: Si , type_b: Si , potential: zero , rcut: 8.47 ang }        - { type_a: Si , type_b:  O , potential: zero , rcut: 5.00 ang }</p>"},{"location":"User/ForceFields/Pair/Strategies/index.html","title":"Strategies","text":"<p>Pair potentials can be used in multiple ways in exaStamp. Depending on whether a single specy or multiple species are present in the system, one can use different variants of a pair potential or even mix them. Below is a rapid presentation of these variants.</p>"},{"location":"User/ForceFields/Pair/Strategies/index.html#single-specy","title":"Single specy","text":"<p>Example: Basic usage of a pair potential</p> <pre><code>lj_compute_force:\n  parameters: { epsilon: 0.0104 eV , sigma: 3.4 ang }\n  rcut: 8.0 ang\n\ncompute_force: lj_compute_force\n</code></pre> <p>Example: Symmetric variant of a pair potential</p> <pre><code>lj_compute_force_symetric:\n  parameters: { epsilon: 0.0104 eV , sigma: 3.4 ang }\n  rcut: 8.0 ang\n\ncompute_force: lj_compute_force\n</code></pre>"},{"location":"User/ForceFields/Pair/Strategies/index.html#multiple-species","title":"Multiple species","text":"<p>Example: Multiple species variant of a pair potential</p> <pre><code>lj_multi_force:\n  rcut: 8. ang\n  common_parameters: { epsilon: 0.0 , sigma: 0.0 }\n  parameters:\n    - { type_a: Zn , type_b: Zn , rcut: 6.10 ang , parameters: { epsilon: 2.522E-20 J , sigma: 0.244E-09 m } }\n    - { type_a: Cu , type_b: Zn , rcut: 5.89 ang , parameters: { epsilon: 4.853E-20 J , sigma: 0.236E-09 m } }\n\ncompute_force: lj_compute_force\n</code></pre>"},{"location":"User/ForceFields/Pair/Strategies/index.html#mixing-pair-potentials","title":"Mixing pair potentials","text":"<p>In the presence of multiple types in the simulated sample, and when applicable, the potentials defined hereafter may be used using the following formalism:</p> <p>Example: Multiple species variant of a pair potential</p> <pre><code>compute_force_pair_multimat:\n  potentials:\n    - { type_a: Si , type_b: Cu , potential: lj , rcut: 5.89 ang , parameters: { epsilon: 1.0 eV , sigma: 2.3 ang } }\n    - { type_a: Si , type_b: Zn , potential: buckingham , rcut: 7.10 ang , parameters: { A: 1.3 eV, Rho: 1.2 ang, C: 1.5 eV*ang^6} }\n    - { type_a: Si  , type_b: Zn, potential: exp6, rcut: 12.5 ang, parameters: { A:  37111.29 Da*kcal/g, B: 3.46350030 1/ang, C: 484.2991571 Da*kcal*ang^6/g, D: 5.0e-5 Da*kcal/g } }  \ncompute_force: lj_compute_force\n</code></pre>"},{"location":"User/ForceFields/Reactive/index.html","title":"Reactive Potentials","text":""},{"location":"User/ForceFields/Reactive/lchbop.html","title":"LCHBOP - Long-range Carbon-Hydrogen Bond Order Potential","text":""},{"location":"User/ForceFields/Reactive/rebo.html","title":"REBO - Reactive Empirical Bond Order Potential","text":""},{"location":"User/GlobalSetup/index.html","title":"<code>global</code> block definition","text":""},{"location":"User/Grids/index.html","title":"Simulation's graph","text":"<p>The default simulation sequence of <code>exaStamp</code> is defined in the <code>main-config.msp</code> file located in <code>exaStamp/data/config</code>. In particular, the <code>simulation</code> block allows for the entire parametrization of the MD simulation, from the initial information output and hardware initialization to the main MD loop and hardware finalization.</p> <p>Default YAML block for an exaStamp simulation</p> <pre><code>simulation:\n  name: MySimulation\n  body:\n    - print_logo_banner                                   # Print the exaStamp banner\n    - hw_device_init                                      # Default communicator + CUDA initialization\n    - make_empty_grid                                     # Create an empty grid\n    - grid_flavor                                         # Define what information is attached to the grid\n    - global                                              # Global simulation controls\n    - init_parameters                                     # Additional control parameters\n    - generate_default_species                            # Generate default species\n    - particle_regions                                    # Geometrical regions definition\n    - preinit_rcut_max                                    # Automatic cell_size calculation\n    - domain                                              # Simulation domain definition\n    - init_prolog                                         # Initialization prologue\n    - input_data                                          # Populate domain with particles\n    - species:                                            # Species definition\n        verbose: false\n        fail_if_empty: true\n    - grid_post_processing                                # Grid memory compaction\n    - reduce_species_after_read                           # Update particle species\n    - init_rcut_max                                       # Update neighborhood distance and displacement tolerance\n    - print_domain                                        # Print Domain information\n    - performance_adviser: { verbose: true }              # Print performance advices\n    - do_init_temperature                                 # Initialize temperature if needed\n    - init_epilog                                         # Initialization epilogue\n    - species:                                            # Species definition recheck\n        verbose: true\n        fail_if_empty: true\n    - first_iteration                                     # Simulation first iteration\n    - compute_loop                                        # Simulation compute loop\n    - simulation_epilog                                   # Simulation finalization\n    - hw_device_finalize                                  # CUDA finalization\n</code></pre> <p>In this YAML block, some operators are mandatory. We provide in the next section a input deck example with the minimal information required. In addition, some of the YAML operators defined in the <code>simulation</code> block are defined in other configuration files located in <code>exaStamp/data/config</code>. However, the mandatory blocks to be defined by the user to build a minimal input deck for exaStamp are:</p> <pre><code>   - global         # Global control of simulation parameters\n   - species        # Definition of the particles' species\n   - compute_force  # Choice of the interatomic potential\n   - domain         # Definition of the simulation's domain\n   - input_data     # Population of the domain with particles\n</code></pre> <p>The next section provides a basic example on how to build an <code>exaStamp</code> input deck with the minimal information required.</p> <pre><code>graph TD\n  A[\"`**Initialization**\n  Detect GPU Support\n  Create Grid\n  Preinit cutoff radius`\"]\n\n  B[Creating Particle Regions]\n\n  C[\"`**Input Data**\n  lattice, xyz, MpiIO\n  + field initialization`\"]\n\n  D[\"`**Firs Timestep**\n  Init partcles\n  - move\n  - migration\n  - neighbor lists\n  - compute forces`\"]\n\n  E[Time Loop];\n  F[test];\n  G[coucou];\n\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt; D;\n  D --&gt; E;\n  E --&gt; F;\n  F --&gt; G\n  G---&gt;|\"`t inf tfin`\"| D;\n\n</code></pre> <p>B --&gt;|Yes| C[Hmm...];   C --&gt; D[Debug];   D --&gt; B;   B ----&gt;|No| E[Yay!];   E ===&gt; newLines[\"<code>Line1                     Line 2                     Line 3</code>\"]  </p>"},{"location":"User/Grids/analysis.html","title":"Analysis","text":""},{"location":"User/Grids/flavors.html","title":"Grid Flavors","text":"<p>In <code>ExaStamp</code>, the grid flavor pre-defines the data attached to particles during the simulation, meaning that such data is stored anyway along the entire trajectory. Some fields can evolve with time such as positions or velocity while some can be fixed such as the particle identifiers or partial charges.</p>"},{"location":"User/Grids/flavors.html#general-concept","title":"General Concept","text":"<p>To define the grid flavor to be used in the simulation, a <code>YAML</code> block can be used in the input file. Below are exemples of definitions for all grid flavors available in <code>ExaStamp</code>.</p> <p>.. code-block:: yaml</p> <p># Minimal    grid_flavor: grid_flavor_minimal</p> <p># Multimat    grid_flavor: grid_flavor_multimat</p> <p># Full    grid_flavor: grid_flavor_full</p> <p># Mechanics    grid_flavor: grid_flavor_mechanics</p> <p># Multimat Mechanics    grid_flavor: grid_flavor_multimat_mechanics</p> <p># Rigid Molecules    grid_flavor: grid_flavor_rigidmol</p> <p>.. warning::</p> <p>Depending on the grid flavor chosen by the user, some fields won't be available for output. For example if one needs the global stress tensor computed by the <code>thermodynamic_state</code> operator, an appropriate <code>grid_flavor</code> to use would be the <code>grid_flavor_full</code>.</p> <p>Amongst these grid flavors, mutliple fields are attached to particles. The following table lists the fields common to all grid flavors</p> <p>.. list-table:: Common fields to all grid flavors    :widths: 30 30    :header-rows: 1    :align: center</p> <ul> <li> <ul> <li>Field</li> <li>Type</li> </ul> </li> <li> <ul> <li>Potential Energy</li> <li>float</li> </ul> </li> <li> <ul> <li>Position</li> <li>Vec3d</li> </ul> </li> <li> <ul> <li>Velocity</li> <li>Vec3d</li> </ul> </li> <li> <ul> <li>Force</li> <li>Vec3d</li> </ul> </li> </ul> <p>Below is another table listing the additional per-particle fields in the different grid flavors. Depending on the application you're thinking about, you may choose the appropriate grid_flavor for your simulation.</p> <p>.. list-table:: Per-particle fields available in the different grid flavors    :widths: 30 30 30 30 30 30 30    :header-rows: 1    :align: center</p> <ul> <li> <ul> <li>Field \\ Grid Flavor</li> <li>mimimal</li> <li>multimat</li> <li>multimat_mechanics</li> <li>full</li> <li>full_mechanics</li> <li>rigidmol</li> </ul> </li> <li> <ul> <li>Filtered Position</li> <li>NO</li> <li>NO</li> <li>YES</li> <li>NO</li> <li>YES</li> <li>NO</li> </ul> </li> <li> <ul> <li>Particle Identifier</li> <li>NO</li> <li>YES</li> <li>YES </li> <li>YES</li> <li>YES </li> <li>YES</li> </ul> </li> <li> <ul> <li>Particle Type</li> <li>NO</li> <li>YES</li> <li>YES</li> <li>YES</li> <li>YES </li> <li>YES</li> </ul> </li> <li> <ul> <li>Virial</li> <li>NO</li> <li>NO</li> <li>YES  </li> <li>YES</li> <li>YES    </li> <li>NO</li> </ul> </li> <li> <ul> <li>Particle Charge</li> <li>NO</li> <li>NO</li> <li>NO  </li> <li>YES</li> <li>NO </li> <li>YES</li> </ul> </li> <li> <ul> <li>Molecule Identifier</li> <li>NO</li> <li>NO</li> <li>NO  </li> <li>YES</li> <li>NO </li> <li>YES</li> </ul> </li> <li> <ul> <li>Cmol</li> <li>NO</li> <li>NO</li> <li>NO  </li> <li>YES</li> <li>NO </li> <li>YES</li> </ul> </li> <li> <ul> <li>Quaternion</li> <li>NO</li> <li>NO</li> <li>NO  </li> <li>NO</li> <li>NO </li> <li>YES</li> </ul> </li> <li> <ul> <li>Angular Momentum</li> <li>NO</li> <li>NO</li> <li>NO  </li> <li>NO</li> <li>NO </li> <li>YES</li> </ul> </li> <li> <ul> <li>Torque</li> <li>NO</li> <li>NO</li> <li>NO  </li> <li>NO</li> <li>NO </li> <li>YES</li> </ul> </li> </ul>"},{"location":"User/Grids/flavors.html#recommendations","title":"Recommendations","text":"<p>Below are some recommendations on using the different grid flavors, depending on the particle types involved in your simulation.</p>"},{"location":"User/Grids/flavors.html#mono-specy-and-multi-species-atomic-systems","title":"Mono-specy and Multi-species Atomic Systems","text":"<p>For mono-species and multi-species atomic systems, the different grid flavors that can be used are the following:</p> <ul> <li><code>grid_flavor_minimal</code> to model system with minimal per-particle data. Not compatible with charged systems.</li> <li><code>grid_flavor_multimat</code> to model multi-species systems and analyse the system using particle identifiers.</li> <li><code>grid_flavor_full</code> to model multi-species systems with per-particle charges and virial.</li> </ul>"},{"location":"User/Grids/flavors.html#rigid-molecules-systems","title":"Rigid Molecules Systems","text":"<p>For rigid molecules system, the only grid flavor that can be used is the <code>grid_flavor_rigidmol</code>.  </p>"},{"location":"User/Grids/flavors.html#flexible-molecules-systems","title":"Flexible Molecules Systems","text":"<p>For systems containing fully flexible molecules, the only grid flavor that can be used is the <code>grid_flavor_full</code>.</p>"},{"location":"User/Grids/input.html","title":"Input","text":""},{"location":"User/Grids/output.html","title":"Output","text":"<p>When it comes to large scale MD simulations it can be very costly to output the entire system to the disk in order to perform post-analysis. ExaNBody offers a way to project particles properties to a regular grid, built on the grid used for paralellism defined by the <code>cell_size</code> parameter. One the properties are projected, a few output operators allow to dump the data using ImageData or UnstructuredGrid VTK formats. As for the discrete output operators, everything can be performed in the block <code>dump_analysis</code> :</p> <p>.. code-block:: yaml</p> <p>dump_analysis:      - project_data_to_grid      - define_timestep_file      - continuum_output_operator</p> <p>Here, the <code>dump_analysis</code> block contains three distinct operators. The first one allows to perform the data projection using the <code>atom_cell_projection</code> operator which takes the following arguments:</p> <ul> <li>fields = List of strings corresponding to the projected fields onto the regular grid</li> <li>grid_subdiv = Subdivision of the parallelism grid</li> <li>splat_size = Distance used to project the data onto the regular grid and calculate each particle's contribution to neighboring cells</li> </ul> <p>The user-defined operator <code>project_data_to_grid</code> can be defined as follows:</p> <p>.. code-block:: yaml</p> <p>project_data_to_grid:      - grid_flavor      - resize_grid_cell_values      - ghost_update_r_v      - atom_cell_projection:          fields: [\"mv2\", \"mass\", \"vnorm\", \"f\"]          grid_subdiv: 2          splat_size: 4.5 ang</p> <p>Where <code>grid_flavor</code> sets the type of grid_flavor to use, <code>resize_grid_cell_values</code> allows to resize the data structure to the existing grid size to perform properties projections and where <code>ghost_update_r_v</code> allows to transfer both positions and velocities to the ghost layers at the domain boundaries and between MPI domains to ensure fields continuity on the projection grid.</p> <p>The user-defined operator <code>define_timestep_file</code> corresponds to the block in which the output file name is defined base on the current iteration as well as some messages that will be printed to the screen when the operator <code>dump_analysis</code> is triggered :</p> <p>.. code-block:: yaml</p> <p>define_timestep_file:                    - timestep_file: \"folder/output_%010d\"      - message: { mesg: \"Write FILE_OUPTUT \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }</p> <p>Finally, we define in the following the different solutions available to replace the <code>continuum_output_operator</code> above. The solutions described here only concern continuum output operators, i.e. operators that outputs particles' properties projected onto a regular grid.</p>"},{"location":"User/Grids/output.html#write_grid_vtk","title":"write_grid_vtk","text":"<p>This is a generic operator common to all applications, i.e. ExaDEM, ExaSTAMP and ExaSPH. It allows to dump particles attached properties on a regular grid that covers the entire simulation domain. Beware, this operator outputs a grid that is not scaled on the real simulation domain lengths. It uses the ImageData structure of VTK and therefore consists in a regular parallelepiped. It takes the unique following argument :</p> <ul> <li>use_point_data = Check what it means.</li> </ul> <p>.. code-block:: yaml</p> <p># Definition of the write_grid_vtk operator    write_grid_vtk:      use_point_data: true</p>"},{"location":"User/Grids/output.html#write_deformed_grid_vtk","title":"write_deformed_grid_vtk","text":"<p>This is also a generic operator common to all applications, i.e. ExaDEM, ExaSTAMP and ExaSPH. It allows to dump particles attached properties on a regular grid that covers the entire simulation domain. This operator generalizes the <code>write_grid_vtk</code> operator to dynamically evolving simulation domains. Indeed, it uses the UnstructuredGrid format from VTK and therefore outputs a non-regular parallelepiped that follows the simulation domain shape with time. It is particularly usefull when applyging dynamic deformations to the simulation domain. The units of the output regular grid are the real units of the simulation domain. It takes the unique following argument :</p> <ul> <li>use_point_data = Check what it means.</li> </ul> <p>.. code-block:: yaml</p> <p># Definition of the write_deformed_grid_vtk operator    write_deformed_grid_vtk:      use_point_data: true</p>"},{"location":"User/Grids/output.html#usage-examples","title":"Usage examples","text":"<p>Let's take a simple case of a voided sample.</p>"},{"location":"User/Particles/index.html","title":"Simulation's graph","text":"<p>The default simulation sequence of <code>exaStamp</code> is defined in the <code>main-config.msp</code> file located in <code>exaStamp/data/config</code>. In particular, the <code>simulation</code> block allows for the entire parametrization of the MD simulation, from the initial information output and hardware initialization to the main MD loop and hardware finalization.</p> <p>Default YAML block for an exaStamp simulation</p> <pre><code>simulation:\n  name: MySimulation\n  body:\n    - print_logo_banner                                   # Print the exaStamp banner\n    - hw_device_init                                      # Default communicator + CUDA initialization\n    - make_empty_grid                                     # Create an empty grid\n    - grid_flavor                                         # Define what information is attached to the grid\n    - global                                              # Global simulation controls\n    - init_parameters                                     # Additional control parameters\n    - generate_default_species                            # Generate default species\n    - particle_regions                                    # Geometrical regions definition\n    - preinit_rcut_max                                    # Automatic cell_size calculation\n    - domain                                              # Simulation domain definition\n    - init_prolog                                         # Initialization prologue\n    - input_data                                          # Populate domain with particles\n    - species:                                            # Species definition\n        verbose: false\n        fail_if_empty: true\n    - grid_post_processing                                # Grid memory compaction\n    - reduce_species_after_read                           # Update particle species\n    - init_rcut_max                                       # Update neighborhood distance and displacement tolerance\n    - print_domain                                        # Print Domain information\n    - performance_adviser: { verbose: true }              # Print performance advices\n    - do_init_temperature                                 # Initialize temperature if needed\n    - init_epilog                                         # Initialization epilogue\n    - species:                                            # Species definition recheck\n        verbose: true\n        fail_if_empty: true\n    - first_iteration                                     # Simulation first iteration\n    - compute_loop                                        # Simulation compute loop\n    - simulation_epilog                                   # Simulation finalization\n    - hw_device_finalize                                  # CUDA finalization\n</code></pre> <p>In this YAML block, some operators are mandatory. We provide in the next section a input deck example with the minimal information required. In addition, some of the YAML operators defined in the <code>simulation</code> block are defined in other configuration files located in <code>exaStamp/data/config</code>. However, the mandatory blocks to be defined by the user to build a minimal input deck for exaStamp are:</p> <pre><code>   - global         # Global control of simulation parameters\n   - species        # Definition of the particles' species\n   - compute_force  # Choice of the interatomic potential\n   - domain         # Definition of the simulation's domain\n   - input_data     # Population of the domain with particles\n</code></pre> <p>The next section provides a basic example on how to build an <code>exaStamp</code> input deck with the minimal information required.</p> <pre><code>graph TD\n  A[\"`**Initialization**\n  Detect GPU Support\n  Create Grid\n  Preinit cutoff radius`\"]\n\n  B[Creating Particle Regions]\n\n  C[\"`**Input Data**\n  lattice, xyz, MpiIO\n  + field initialization`\"]\n\n  D[\"`**Firs Timestep**\n  Init partcles\n  - move\n  - migration\n  - neighbor lists\n  - compute forces`\"]\n\n  E[Time Loop];\n  F[test];\n  G[coucou];\n\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt; D;\n  D --&gt; E;\n  E --&gt; F;\n  F --&gt; G\n  G---&gt;|\"`t inf tfin`\"| D;\n\n</code></pre> <p>B --&gt;|Yes| C[Hmm...];   C --&gt; D[Debug];   D --&gt; B;   B ----&gt;|No| E[Yay!];   E ===&gt; newLines[\"<code>Line1                     Line 2                     Line 3</code>\"]  </p>"},{"location":"User/Particles/analysis.html","title":"Analysis","text":"<p>.. _particles-analysis:</p>"},{"location":"User/Particles/analysis.html#analysis","title":"Analysis","text":""},{"location":"User/Particles/analysis.html#local-entropy","title":"Local entropy","text":"<p>The <code>compute_local_entropy</code> operator computes the per-atom entropy as define in ref.</p> <p>.. code-block:: yaml</p> <p>compute_local_entropy:      rcut: 5.0 ang      sigma: 0.15 ang      local: true      nbins: 50</p> <p>.. list-table::    :widths: 10 40 10 10    :header-rows: 1</p> <ul> <li> <ul> <li>Property</li> <li>Description</li> <li>Data Type</li> <li>Default</li> </ul> </li> <li> <ul> <li><code>rcut</code></li> <li>Upper integration limit of Eq. :eq:<code>eq_per_atom_entropy</code>. Distance unit.</li> <li>float</li> <li>\\( r_c^{max} \\)</li> </ul> </li> <li> <ul> <li><code>sigma</code></li> <li>Broadening parameter. Distance unit.</li> <li>float</li> <li>:math:<code>0.1 \\, \\AA</code></li> </ul> </li> <li> <ul> <li><code>local</code></li> <li>If set to <code>true</code> , the \\( g_m^i \\) is normalized by the local density around atom \\( i \\).</li> <li>bool</li> <li><code>true</code></li> </ul> </li> <li> <ul> <li><code>nbins</code></li> <li>Number of bin on which the integral is discretised.</li> <li>int</li> <li>\\(n = \\lfloor r_{c} / \\sigma \\rfloor \\)</li> </ul> </li> </ul> <p>The per-atom entropy \\( s^{i}_{S} \\) is computed using the following formula:</p> <p>.. math::     eq_per_atom_entropy</p> <pre><code>s^{i}_{S} = -2 \\pi \\rho k_{B} \\int_{0}^{r_{m}} \\left[ g^i_m(r) \\ln g^i_m(r) - g^i_m(r) + 1 \\right] r^2 dr\n</code></pre> <p>where \\( g^i_m(r) \\) is a mollified version of the radial distribution function</p> <p>.. math::     grm</p> <pre><code>g^i_m(r) = \\frac{1}{4 \\pi \\rho r^2} \\sum_{j \\neq i} \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2}}\\exp^{-(r - r_{ij})^2 / (2 \\sigma^2)}\n</code></pre> <p>where \\( r_{ij} \\) is the distance between central atom \\( i \\) and its neighbour \\( j \\), and \\( \\sigma \\) is the smoothing parameter.</p>"},{"location":"User/Particles/analysis.html#local-centrosymmetry","title":"Local centrosymmetry","text":"<p>The <code>compute_local_centrosymmetry</code> operator computes the per-atom entropy as define in ref.</p> <p>.. code-block:: yaml</p> <p>compute_local_centrosymmetry:      rcut: 5.0 ang      nnn:  8</p>"},{"location":"User/Particles/input.html","title":"Input","text":"<p>.. _particles-input:</p>"},{"location":"User/Particles/input.html#input","title":"Input","text":"<p>The present section describes in details the different operators used for simulation's setup, post-analysis, visualization and I/O operations in Molecular Dynamics simulations using exaStamp.</p> <p>.. code-block:: yaml</p> <p>input_data:      - domain      - init_rcb_grid      - particle_regions      - lattice</p> <p>.. _builtin-particles:</p>"},{"location":"User/Particles/input.html#built-in-particle-creation","title":"Built-in particle creation","text":"<p>.. _input-lattice:</p> <p>Lattice generator</p> <p>.. _input-bulk-lattice:</p> <p>Bulk lattice generator</p> <p>.. _external-readers:</p>"},{"location":"User/Particles/input.html#reading-external-files","title":"Reading external files","text":"<p>.. _input-read-dump-atoms:</p> <p>Readers of restart files for atomic systems</p> <p>.. _input-read-dump-mol:</p> <p>Readers of restart files for flexible molecules systems</p> <p>.. _input-read-dump-rigidmol:</p> <p>Readers of restart files for rigid molecules systems</p> <p>.. _input-read-xyz-xform:</p> <p>Readers of xyz File</p> <ul> <li>Name: <code>read_xyz</code></li> <li>Description: This operator reads a file written according to the xyz format.</li> <li>Parameters:</li> <li><code>bounds_mode</code> : default mode corresponde to ReadBoundsSelectionMode.</li> <li><code>enlarge_bounds</code> : Define a layer around the volume size in the xyz file. Default size is 0.</li> <li><code>file</code> : File name, this parameter is required.</li> <li><code>pbc_adjust_xform</code> : Ajust the form.</li> </ul> <p>Reading external file formats</p> <p>.. warning::     Only supported for atomic systems</p> <p>.. warning::     If multiple structures are present in the file, the operator will always read the first one.</p> <p>In addition to :ref:<code>builtin-particles creation &lt;builtin-particles&gt;</code> and :ref:<code>restart files&lt;input-read-dump-atoms&gt;</code>, <code>ExaStamp</code> can read external file formats though the <code>read_external_file_format</code> operator.</p> <p>.. code-block:: yaml</p> <p>read_external_file_format:      file: /path/to/example-file.xyz.gz      format: xyz      compression: gz      units_style: metal      style_style: full</p> <p>.. list-table::    :widths: 10 40 10    :header-rows: 1</p> <ul> <li> <ul> <li>Property</li> <li>Description</li> <li>Data Type</li> </ul> </li> <li> <ul> <li><code>file</code></li> <li>File name</li> <li>string</li> </ul> </li> <li> <ul> <li><code>format</code></li> <li>File format extension (see :ref:<code>supported format&lt;input-supported-ext-format&gt;</code>)</li> <li>string</li> </ul> </li> <li> <ul> <li><code>compression</code></li> <li>File compression extension (<code>gz</code> , <code>bz2</code> , <code>xz</code>)</li> <li>string</li> </ul> </li> <li> <ul> <li><code>units_style</code></li> <li>LAMMPS units style (only used for LAMMPS format)</li> <li>string</li> </ul> </li> <li> <ul> <li><code>atom_style</code></li> <li>LAMMPS atom style (only used for LAMMPS format)</li> <li>string</li> </ul> </li> </ul> <p>Only the <code>file</code> parameters is required. By default, <code>format</code> and <code>compression</code> are deduced from the file's extension. The <code>units_style</code> and <code>atom_style</code> parametyers are used only with LAMMPS format to define the unit system and the atom style.</p> <p>.. _input-supported-ext-format:</p> <p>Supported file formats</p> <p>Currently <code>ExaStamp</code> support the following external file formats:</p> <p>.. list-table::    :widths: 40 40 40    :header-rows: 1</p> <ul> <li> <ul> <li>Name</li> <li>Description</li> <li>Extension</li> </ul> </li> <li> <ul> <li>LAMMPS data</li> <li>File format used by <code>LAMMPS &lt;https://docs.lammps.org/Run_formats.html#input-file&gt;</code>_</li> <li><code>lmp</code>, <code>lmp-data</code>, <code>data</code></li> </ul> </li> <li> <ul> <li>LAMMPS Dump</li> <li>File format used by <code>LAMMPS &lt;https://docs.lammps.org/Run_formats.html#input-file&gt;</code>_</li> <li><code>dump</code>, <code>lmp-dump</code></li> </ul> </li> <li> <ul> <li>XYZ</li> <li><code>Extended XYZ &lt;https://github.com/libAtoms/extxyz?tab=readme-ov-file#xyz-file&gt;</code>_ format.</li> <li><code>xyz</code></li> </ul> </li> </ul>"},{"location":"User/Particles/output.html","title":"Output","text":"<p>.. _particles-output:</p>"},{"location":"User/Particles/output.html#output","title":"Output","text":"<p>In order to post-process or visualize the particles with time, multiple solutions exist in ExaSTAMP. The general operator that defines the output is named <code>dump_analysis</code> and can be added as a general YAML block in the simulation input file. In this block, the name of the output files can be defined as well as some log message if needed. These properties are then passed as input to the desired output operator:</p> <p>.. code-block:: yaml</p> <p>dump_analysis:      - timestep_file: \"folder/output_%010d\"      - message: { mesg: \"Write FILE_OUPTUT \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - discrete_output_operator</p> <p>It is to be noted that in the block <code>dump_analysis</code> the operator <code>timestep_file</code> is called. This operator takes format as an input and automatically generates the variable <code>filename</code> that is systematically required by all output operators. If the operator <code>timestep_file</code> is not called prior to the output operator, thne the latter requires the <code>filename</code> variable to be defined. We define in the following the different solutions available to replace the <code>discrete_output_operator</code> above. The solutions described here only concern discrete output operators, i.e. operators that outputs explicitely the particles positions with their attached properties. Other operators, that allow to project particles properties onto regular grids will be described in the section <code>Continuum output operators</code>.</p>"},{"location":"User/Particles/output.html#write_paraview_generic","title":"write_paraview_generic","text":"<p>This is a generic operator common to all applications, i.e. ExaDEM, ExaSTAMP and ExaSPH. It allows to dump particles positions and attached properties in parallel using the vtp format that can be read and processed using ParaView. It takes the following arguments:</p> <ul> <li><code>binary_mode</code> = allows to write the paraview files in binary mode with a certain compression level. default value: true</li> <li><code>compression</code> = compression level for the binary_mode, default value: 'default'. default value: 'default'</li> <li><code>write_box</code> = outputs the paraview file that contains the box associated to the domain. default value: 'false'</li> <li><code>write_ghost</code> = outputs the ghost particles around the domain. default value: 'false'</li> <li><code>fields</code> =  List of strings corresponding to grid fields to dump as particle's attributes. Particles positions and ids are dumped by default.</li> </ul> <p>YAML usage example:</p> <p>.. code-block:: yaml</p> <p># General dump_analysis operator, called each timestep defined with the anaysis_dump_frequency keyword    dump_analysis:      - timestep_file: \"paraview/output_%010d\"      - message: { mesg: \"Write paraview \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_paraview_generic</p> <p># Definition of the write_paraview_generic operator    write_paraview_generic:      binary_mode: false      write_ghost: false      write_box: true      fields: [\"type\", \"vx\",\"vy\",\"vz\", \"ep\"]</p>"},{"location":"User/Particles/output.html#write_paraview","title":"write_paraview","text":"<p>This is an operator specific to ExaSTAMP and very similar to the operator <code>write_paraview_generic</code> that might be deleted soon. It allows to dump particles positions and attached properties in parallel using the vtp format that can be read and processed using ParaView. It takes the following arguments:</p> <ul> <li><code>binary_mode</code> = allows to write the paraview files in binary mode with a certain compression level. default value: true</li> <li><code>compression</code> = compression level for the binary_mode, default value: 'default'. default value: 'default'</li> <li><code>write_box</code> = outputs the paraview file that contains the box associated to the domain. default value: 'false'</li> <li><code>write_ghost</code> = outputs the ghost particles around the domain. default value: 'false'</li> <li><code>fields</code> =  List of strings corresponding to grid fields to dump as particle's attributes. Particles positions and ids are dumped by default.</li> </ul> <p>YAML usage example:</p> <p>.. code-block:: yaml</p> <p># General dump_analysis operator, called each timestep defined with the anaysis_dump_frequency keyword    dump_analysis:      - timestep_file: \"paraview/output_%010d\"      - message: { mesg: \"Write paraview \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_paraview</p> <p># Definition of the write_paraview_generic operator    write_paraview:      binary_mode: false      write_ghost: false      write_box: true      fields: [\"type\", \"vx\",\"vy\",\"vz\", \"ep\"]</p>"},{"location":"User/Particles/output.html#write_xyz","title":"write_xyz","text":"<p>Allows to dump particles positions, types and ids in a .xyz file. This operator does not allow to dump other attached properties. A Generic xyz file writer will added soon to exaNBody such that all variants ExaDEM, ExaSTAMP, ExaSPH can beneficiate from it.</p> <ul> <li>Operator name =  <code>write_xyz</code></li> </ul> <p>YAML usage example:</p> <p>.. code-block:: yaml</p> <p># General dump_analysis operator, called each timestep defined with the anaysis_dump_frequency keyword    dump_analysis:      - timestep_file: \"xyz/output_%010d\"      - message: { mesg: \"Write xyz \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_xyz</p>"},{"location":"User/Particles/output.html#write_lmp","title":"write_lmp","text":"<p>Allows to dump particles positions, types and ids in a .lmp file. This operator does not allow to dump other attached properties. A Generic LMP file writer will added soon to exaNBody such that all variants ExaDEM, ExaSTAMP, ExaSPH can beneficiate from it.</p> <ul> <li>Operator name =  <code>write_lmp</code></li> </ul> <p>YAML usage example:</p> <p>.. code-block:: yaml</p> <p># General dump_analysis operator, called each timestep defined with the anaysis_dump_frequency keyword    dump_analysis:      - timestep_file: \"xyz/output_%010d\"      - message: { mesg: \"Write xyz \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_lmp</p>"},{"location":"User/Particles/output.html#write_vtklegacy","title":"write_vtklegacy","text":"<p>This is a generic operator common to all applications, i.e. ExaDEM, ExaSTAMP and ExaSPH. It allows to dump particles positions and all attached grid properties in parallel using the vtp format that can be read and processed using ParaView. It takes the following arguments:</p> <ul> <li><code>ghost</code> = outputs the ghost particles around the domain. default value: 'false'</li> <li><code>ascii</code> = outputs the data in ascii format. default value: 'false'</li> </ul> <p>YAML usage example:</p> <p>.. code-block:: yaml</p> <p># General dump_analysis operator, called each timestep defined with the anaysis_dump_frequency keyword    dump_analysis:      - timestep_file: \"paraview/output_%010d\"      - message: { mesg: \"Write paraview \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_vtklegacy</p> <p># Definition of the write_paraview_generic operator    write_vtklegacy:      ascii: true      ghost: false</p>"},{"location":"User/Particles/species.html","title":"Species","text":"<p>.. _particles-species:</p>"},{"location":"User/Particles/species.html#species","title":"Species","text":"<p>Multiple particle types can be used in <code>ExaStamp</code> such as atoms, rigid molecules or fully-flexible molecules. These three formalisms have an influence on the way the interactions are computed which will be explained in detail below. However, the common basis for all particle types is the definition of a <code>YAML</code> block containing information about the atoms themselves. The particles properties are initialized by the <code>species</code> operator that must be appended in the YAML input file:</p> <p>.. code-block:: yaml    :caption: YAML block for particles type definition</p> <p># 1st solution: Species definition for atomic systems    species:      - O:          mass:  15.9994 Da          z: 8          charge: -1.1104 e-      - U:          mass:  238.02891 Da          z: 92          charge: 2.2208 e-</p> <p># 2nd solution: Species definition for molecular systems    species:      - h2o_H:          mass:  1.008 Da          z: 1          charge: 0.5564 e-          molecule: molH20      - h2o_O:          mass:  15.999 Da          z: 8          charge: -1.1128 e-          molecule: molH20      - o2_O:          mass:  15.999 Da          z: 8          charge: 0.0 e-          molecule: molO2</p> <p>.. warning::</p> <p>The particle types enumerated in the <code>species</code> YAML block must be consistent with particle types that are read in external files, e.g. when reading a <code>.xyz</code> file generated with an external software. In addition, the <code>lattice</code> operator needs the user to assign particle species associated with the template used (e.g. SC, BCC, FCC etc). These species must also be consistent with the <code>species</code> block. See :ref:<code>input-lattice</code> for further details.</p> <p>The <code>species</code> YAML block contains a sequence of particle types that contain different properties explained below.</p> <p>.. list-table:: Properties for each particle specy    :widths: 40 40 40    :header-rows: 1</p> <ul> <li> <ul> <li>Property</li> <li>Data Type</li> <li>Example</li> </ul> </li> <li> <ul> <li><code>mass</code></li> <li>float</li> <li> <p>.. code-block:: yaml</p> <p>mass: 15.9994 Da    * - <code>z</code>      - int      - .. code-block:: yaml</p> <p>z: 8    * - <code>charge</code>      - float      - .. code-block:: yaml</p> <p>charge: -1.1104 e-    * - <code>molecule</code>      - string      - .. code-block:: yaml</p> <p>molecule: molH2O</p> </li> </ul> </li> </ul>"},{"location":"User/SimulationGraph/index.html","title":"Understanding the simulation's graph","text":""},{"location":"User/SimulationGraph/DefaultYAML.html","title":"Default configuration","text":"<p>Configuration files for the default behavior of <code>exaStamp</code> can be found in the <code>exaStamp/data/config</code> folder. These files contain all the YAML block required for an MD simulation to work correctly. We'll go over all of them in detail below where each section is associated with one configuration file, starting with the master one.</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#main-configmsp","title":"<code>main-config.msp</code>","text":"<p>.. code-block:: YAML</p> <p>print_logo_banner:      profiling: false      body:        - message: |</p> <pre><code>      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557     \u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n      \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2588\u2588\u2557\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557    \u2588\u2588\u2551   \u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2557\n      \u2588\u2588\u2588\u2588\u2588\u2557   \u255a\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d    \u2588\u2588\u2551   \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2554\u255d\n      \u2588\u2588\u2554\u2550\u2550\u255d   \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d     \u255a\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2588\u2588\u2557 \n      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u255d \u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551          \u255a\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n      \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d           \u255a\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d\n   - version_info\n</code></pre> <p>includes:      - config_defaults.msp      - config_debug.msp      - config_move_particles.msp      - config_numerical_schemes.msp      - config_globals.msp      - config_iteration_log.msp      - config_iteration_dump.msp      - config_end_iteration.msp      - config_init_temperature.msp      - config_input.msp</p> <p># set various defaults    memory_stats: { graph_res_mem: false }</p> <p># usually outputs ParticleSpecies type to a slot named species, in case the data reader doesn't provide species description    reduce_species_after_read: nop</p> <p>############### compute loop stop condition #############    compute_loop_stop:      profiling: false      rebind: { end_at: simulation_end_iteration , result: compute_loop_continue }      body:        - sim_continue</p> <p># default force computation does nothing, these are placeholders used later on    compute_loop_prolog: nop    compute_loop_epilog: nop    init_prolog: nop    init_epilog: nop</p> <p># executing potential sub nodes initializes rcut_max    # won't actually compute anything since grid is empty at this time    init_rcut_max:      profiling: false      rebind: { grid: null_grid, chunk_neighbors: null_neighbors }      body:        - compute_force    # executed on an empty grid, just here to update rcut_max        - nbh_dist:        # compute neighborhood distance in grid space (nbh_dist),            verbose: true  # also computes maximum particle move tolerance in grid space (max_displ)                           # and ghost_dist, a constant scaling of of nbh_dist using scale factor nbh_ghost_scale</p> <p># executed once (whitout verbosity) first to enable automatic cell_size calculation    preinit_rcut_max:      profiling: false      rebind: { grid: null_grid, chunk_neighbors: null_neighbors, domain: empty_domain }      body:        - domain:            cell_size: 0.0 ang            grid_dims: [ 0 , 0 , 0 ]            periodic: [ true , true , true ]        - compute_force        - nbh_dist</p> <p>first_iteration:      - init_particles      - compute_all_forces_energy      - default_thermodynamic_state      - default_print_thermodynamic_state: { lb_flag: false , move_flag: false , print_header: true }      - default_dump_thermodynamic_state      - next_time_step</p> <p># definin this enables user to use --set-init_cuda-enable_cuda false to disable cuda    init_cuda:      enable_cuda: true</p> <p>hw_device_init:      - mpi_comm_world      - init_cuda</p> <p>hw_device_finalize:      - finalize_cuda</p> <p>simulation_epilog:      - default_thermodynamic_state      - final_dump</p> <p># by default, species use the default definition of species operator to generate initial species    species: { species: [ ] , verbose: false }    generate_default_species: species</p> <p>compute_loop:      loop: true      name: loop      condition: compute_loop_continue      body:        - compute_loop_prolog        - begin_iteration        - numerical_scheme        - end_iteration        - compute_loop_epilog        - next_time_step        - compute_loop_stop</p> <p>particles_regions: []</p> <p>init_parameters: nop</p> <p>######## simulation program description #################    simulation:      name: sim      body:        - print_logo_banner        - hw_device_init   # provide MPI_COMM_WORLD as a default communicator        - make_empty_grid        - grid_flavor        - global        - init_parameters        - generate_default_species        - particle_regions        - preinit_rcut_max        - domain        - init_prolog        - input_data        - species: { verbose: false , fail_if_empty: true }        - grid_post_processing        - reduce_species_after_read        - init_rcut_max        - print_domain        - performance_adviser: { verbose: true }        - do_init_temperature        - init_epilog        - species: { verbose: true , fail_if_empty: true }        - first_iteration        - compute_loop        - simulation_epilog        - hw_device_finalize</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_defaultsmsp","title":"<code>config_defaults.msp</code>","text":"<p>The <code>config_defaults.msp</code> file contains the <code>configuration</code> YAML block that allows the user to parametrize different things:</p> <ul> <li><code>physics</code>: definiton of the simulation units </li> <li><code>logging</code>: </li> <li>profiling:</li> <li>debug:</li> <li>mpimt:</li> <li>pinethreads:</li> <li>num_threads:</li> <li>omp_max_nesting: </li> </ul> <p>.. code-block:: YAML</p> <p>configuration:      physics:        units:          length: angstrom          mass: Dalton          time: picosecond          charge: elementary_charge          temperature: kelvin          amount: particle          luminosity: candela          angle: radian          energy: joule      logging:        parallel: false        debug: false        profiling: false      profiling:        exectime: false        summary: false      debug:        plugins: false        config: false        graph: false        graph_lod: 0        filter: []      mpimt: true      pinethreads: false      num_threads: {}      omp_max_nesting: 2</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_debugmsp","title":"<code>config_debug.msp</code>","text":"<p>.. code-block:: YAML</p> <p># debug operators</p> <p>dbg:      - simulation_thermodynamic_state      - print_thermodynamic_state      - dump_thermodynamic_state      - grid_stats</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_move_particlesmsp","title":"<code>config_move_particles.msp</code>","text":"<p>.. code-block:: YAML</p> <p>########### Generic particle update block ################</p> <p>includes:      - config_load_balance.msp</p> <p># define a test node which outputs a boolean value 'trigger_move_particles'    # which tells when to move particles across cells and update neighbor lists    trigger_move_particles:      rebind: { threshold: max_displ , result: trigger_move_particles }      body:        - particle_displ_over</p> <p>################### AMR ############################    rebuild_amr:      sub_grid_density: 6.5      amr_z_curve: false      enforced_ordering: 1    ####################################################</p> <p>############ default ghost update parameters #######    ghost_update_r: &amp;ghost_update_parms      gpu_buffer_pack: true      async_buffer_pack: true      staging_buffer: true      serialize_pack_send: true      wait_all: false</p> <p>ghost_update_rq: *ghost_update_parms</p> <p>update_force_energy_from_ghost: *ghost_update_parms</p> <p>ghost_update_all_no_fv: *ghost_update_parms    ####################################################</p> <p>################### Neighbor list update ############################    chunk_neighbors:      config:        free_scratch_memory: false        build_particle_offset: true        subcell_compaction: true        scratch_mem_per_cell: 1048576        stream_prealloc_factor: 1.05 # standard value to avoid most of dynamic allocations        chunk_size: 1</p> <p>chunk_neighbors_impl: chunk_neighbors</p> <p>update_particle_neighbors:      - amr_grid_pairs      - chunk_neighbors_impl      - resize_particle_locks    ####################################################################</p> <p>grid_post_processing: grid_memory_compact</p> <p>profile_ghost_comm_scheme: nop    #profile_ghost_comm_scheme: print_ghost_comm_stats # exchange volume stats    #profile_ghost_comm_scheme: print_ghost_comm_scheme # full print</p> <p>ghost_update_all_impl: ghost_update_all_no_fv</p> <p>ghost_full_update:      - ghost_comm_scheme      - profile_ghost_comm_scheme      - ghost_update_all_impl</p> <p>################### parallel particle migration ############################    parallel_update_particles:      - migrate_cell_particles      - rebuild_amr      - backup_r      - ghost_full_update      - grid_post_processing      - update_particle_neighbors</p> <p># define actions to initialize particles at startup, just after file read    init_particles:      - move_particles      - extend_domain      - load_balance      - parallel_update_particles    ###########################################################################</p> <p>update_particles_full_body:      - move_particles      - trigger_load_balance      - load_balancing_if_triggered      - parallel_update_particles</p> <p>update_particles_full:      condition: trigger_move_particles      body:        - update_particles_full_body</p> <p>update_particles_fast_body:        - ghost_update_r</p> <p>update_particles_fast:      condition: not trigger_move_particles      body:        - update_particles_fast_body</p> <p>check_and_update_particles:      - trigger_move_particles      - update_particles_full      - update_particles_fast      - loadbalance_log_helper:          rebind: { lb_flag: trigger_load_balance , move_flag: trigger_move_particles }          body: [ lb_event_counter ]</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_numerical_schemesmsp","title":"<code>config_numerical_schemes.msp</code>","text":"<p>.. code-block:: YAML</p> <p>compute_force: nop</p> <p># default is no thermostat    thermostat: nop</p> <p>compute_force: nop</p> <p>compute_force_prolog: zero_force_energy    compute_force_epilog: force_to_accel</p> <p>compute_all_forces_energy:      - compute_force_prolog      - compute_force      - compute_force_epilog</p> <p>verlet_first_half:      - push_f_v_r: { dt_scale: 1.0 , xform_mode: INV_XFORM }      - push_f_v: { dt_scale: 0.5  , xform_mode: IDENTITY }  </p> <p>verlet_second_half:      - push_f_v: { dt_scale: 0.5 , xform_mode: IDENTITY }</p> <p># define the verlet numerical scheme    numerical_scheme_verlet:      name: scheme      body:        - verlet_first_half        - check_and_update_particles        - load_balance_auto_tune_start        - compute_all_forces_energy        - verlet_second_half        - thermostat        - load_balance_auto_tune_end</p> <p># define the verlet numerical scheme    numerical_scheme_basic:      - compute_all_forces_energy      - push_f_v: { xform_mode: IDENTITY }      - push_v_r: { xform_mode: INV_XFORM }      - check_and_update_particles</p> <p>numerical_scheme_overdamped:      - compute_all_forces_energy      - push_f_r: { xform_mode: INV_XFORM }      - check_and_update_particles</p> <p>numerical_scheme: numerical_scheme_verlet</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_globalsmsp","title":"<code>config_globals.msp</code>","text":"<p>.. code-block:: YAML</p> <p># default global values    global:      dt: 1.0e-3 ps      rcut_inc: 1.0 ang   # additional distance so that we d'ont have to rebuild neighbor list each time step      compute_loop_continue: true      simulation_end_iteration: 10000      simulation_log_frequency: 10      simulation_load_balance_frequency: 100      simulation_dump_frequency: 1000         # 0 means no dump at all      simulation_dump_thermo_frequency: 10      analysis_dump_frequency: 0      trigger_thermo_state: true      timestep: 0      physical_time: 0.      init_temperature: -1.0      scale_temperature: -1.0      enable_load_balance: true      enable_task_graph: false      enable_grid_compact: true      trigger_cost_model_fitting: false      cost_model_coefs: [ 0.0 , 0.0 , 1.0 , 0.0 ]      log_mode: mechanical # set to chemistry or default</p> <p># default grid variant we use    grid_flavor: grid_flavor_full</p> <p>make_empty_grid:      rebind: { grid: null_grid }      body:        - grid_flavor</p> <p># default domain parameters    domain:      grid_dims: [ 0 , 0 , 0 ] # deduced from cell_size      cell_size: 0.0 ang # deduced from max rcut (a.k.a. output slot ghost_dist from nbh_dist node)</p> <p>particle_regions: []</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_interation_logmsp","title":"<code>config_interation_log.msp</code>","text":"<p>.. code-block:: YAML</p> <p># default operators to use    default_print_thermodynamic_state: print_thermodynamic_state    default_dump_thermodynamic_state: dump_thermodynamic_state</p> <p># define when log must be printed    trigger_print_log:      rebind: { freq: simulation_log_frequency , result: trigger_print_log , lb_flag: trigger_load_balance , move_flag: trigger_move_particles }      body:        - nth_timestep: { first: true }</p> <p># how to print log    print_log_if_triggered:      condition: trigger_print_log      body:        - default_print_thermodynamic_state: { print_header: false }</p> <p># when to output thermo variables curve    dump_thermo_if_triggered:      condition: trigger_dump_thermo      body:        - default_dump_thermodynamic_state: { print_header: false }</p> <p>trigger_dump_thermo:      rebind: { freq: simulation_dump_thermo_frequency , result: trigger_dump_thermo }      body:        - nth_timestep: { first: false }</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_iteration_dumpmsp","title":"<code>config_iteration_dump.msp</code>","text":"<p>.. code-block:: YAML</p> <p># define when log must be printed    trigger_dump_data:      rebind: { freq: simulation_dump_frequency , result: trigger_dump_data }      body:        - nth_timestep: { first: false }</p> <p>trigger_dump_analysis:      rebind: { freq: analysis_dump_frequency , result: trigger_dump_analysis }      body:        - nth_timestep: { first: false }</p> <p>dump_data_exastamp_v4:      rebind: { thermodynamic_state: thermodynamic_state_4_dump }      body:        - timestep_file: \"ExaStampV4prot_%09d.MpiIO\"        - message: { mesg: \"Write dump \" , endl: false }        - print_dump_file:            rebind: { mesg: filename }            body:              - message: { endl: true }        - simulation_thermodynamic_state        - write_exastamp_v4</p> <p>dump_data_stamp_v4:      rebind: { thermodynamic_state: thermodynamic_state_4_dump }      body:        - timestep_file: \"StampV4prot_%09d.MpiIO\"        - message: { mesg: \"Write dump \" , endl: false }        - print_dump_file:            rebind: { mesg: filename }            body:              - message: { endl: true }        - simulation_thermodynamic_state        - write_stamp_v4</p> <p>dump_data_stamp_v3:      - timestep_file: \"StampV3prot_%09d.MpiIO\"      - message: { mesg: \"Write dump \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_stamp_v3</p> <p># Native ExaNB dump format for single atoms    dump_data_atoms:      - timestep_file: \"atoms_%09d.MpiIO\"      - write_dump_atoms</p> <p># Native ExaNB dump format for rigid molecules    dump_data_rigidmol:      - timestep_file: \"rigidmol_%09d.MpiIO\"      - write_dump_rigidmol</p> <p>dump_data_vtklegacy:      - timestep_file: \"output_%09d.vtk\"      - message: { mesg: \"Write vtk-legacy \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_vtklegacy: { ghost: false }</p> <p>dump_data_vtk:      - timestep_file: \"output_%09d\"      - message: { mesg: \"Write vtk \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_vtk</p> <p>dump_data_paraview:      - timestep_file: \"paraview/output_%09d\"      - message: { mesg: \"Write \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_paraview</p> <p>dump_data_grid_vtklegacy:      - grid_flavor      - resize_grid_cell_values      - atom_cell_projection      - timestep_file: \"grid_%09d.vtk\"      - write_grid_vtklegacy</p> <p>dump_data_grid_vtk:      - grid_flavor      - resize_grid_cell_values      - atom_cell_projection      - timestep_file: \"grid_%09d\"      - write_grid_vtk</p> <p>#dump_data: dump_data_stamp_v3    dump_data: dump_data_atoms</p> <p>dump_data_CCL:      - grid_flavor      - resize_grid_cell_values      - atom_cell_projection      - cc_label      - timestep_file: \"cc_%09d\"      - write_grid_vtk      - write_cc_table</p> <p>dump_data_xyz:      - timestep_file: \"exaStamp_%09d.xyz\"      - message: { mesg: \"Write xyz \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_xyz_file</p> <p>dump_data_lmp:      - timestep_file: \"exaStamp_%09d.lmp\"      - message: { mesg: \"Write xyz \" , endl: false }      - print_dump_file:          rebind: { mesg: filename }          body:            - message: { endl: true }      - write_lmp_file</p> <p>dump_analysis: dump_data_grid_vtk</p> <p># usefull to cleanly place particles in corresponding cells    # and/or extend domain, just before a dump    cleanup_before_dump: init_particles</p> <p># define how to print log    dump_data_if_triggered:      condition: trigger_dump_data      body:    #    - cleanup_before_dump        - dump_data</p> <p>dump_analysis_if_triggered:      condition: trigger_dump_analysis      body:        # - cleanup_before_dump        - dump_analysis</p> <p>final_dump:    #    - cleanup_before_dump        - dump_data</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_end_iterationmsp","title":"<code>config_end_iteration.msp</code>","text":"<p>.. code-block:: YAML</p> <p># define what to do at the end of an iteration</p> <p>trigger_thermo_state:      - combine1:          rebind: { in1: trigger_dump_data , in2: trigger_print_log , result: output1 }          body:            - boolean_or      - combine2:          rebind: { in1: output1 , in2: trigger_dump_thermo , result: trigger_thermo_state }          body:            - boolean_or</p> <p>default_thermodynamic_state: simulation_thermodynamic_state</p> <p>thermo_state_if_triggered:      condition: trigger_thermo_state      body:    #    - message: \"thermodynamic_state\"        - default_thermodynamic_state</p> <p>begin_iteration:      - trigger_dump_data      - trigger_print_log      - trigger_dump_thermo      - trigger_thermo_state      - trigger_dump_analysis</p> <p>end_iteration:      - thermo_state_if_triggered      - dump_data_if_triggered      - print_log_if_triggered      - dump_thermo_if_triggered      - dump_analysis_if_triggered</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_init_temperaturemsp","title":"<code>config_init_temperature.msp</code>","text":"<p>.. code-block:: YAML</p> <p>do_init_temperature:      - block1:          rebind: { value: init_temperature , result: enable_init_temperature }          body:            - greater_than: { threshold: 0.0 K }      - block2:          rebind: { value: scale_temperature , result: enable_scale_temperature }          body:            - greater_than: { threshold: 0.0 K }      - block3:          condition: enable_init_temperature          rebind: { temperature: init_temperature }          body:            - gaussian_noise_v            - init_temperature      - block4:          condition: enable_scale_temperature          rebind: { temperature: scale_temperature }          body:            - init_temperature</p>"},{"location":"User/SimulationGraph/DefaultYAML.html#config_inputmsp","title":"<code>config_input.msp</code>","text":"<p>.. code-block:: YAML</p> <p># default input : read last dump with legacy StampV3 format     read_stamp_v3:          file: lastLegacyDump          filename: lastLegacyDump          bounds_mode: FILE</p> <p>input_data: read_stamp_v3</p>"},{"location":"User/SimulationGraph/simulation_flowchart.html","title":"Simulation flowchart","text":"<p>The default simulation sequence of <code>exaStamp</code> is defined in the <code>main-config.msp</code> file located in <code>exaStamp/data/config</code>. In particular, the <code>simulation</code> block allows for the entire parametrization of the MD simulation, from the initial information output and hardware initialization to the main MD loop and hardware finalization.</p> <p>Default YAML block for an exaStamp simulation</p> <pre><code>simulation:\n  name: MySimulation\n  body:\n    - print_logo_banner                                   # Print the exaStamp banner\n    - hw_device_init                                      # Default communicator + CUDA initialization\n    - make_empty_grid                                     # Create an empty grid\n    - grid_flavor                                         # Define what information is attached to the grid\n    - global                                              # Global simulation controls\n    - init_parameters                                     # Additional control parameters\n    - generate_default_species                            # Generate default species\n    - particle_regions                                    # Geometrical regions definition\n    - preinit_rcut_max                                    # Automatic cell_size calculation\n    - domain                                              # Simulation domain definition\n    - init_prolog                                         # Initialization prologue\n    - input_data                                          # Populate domain with particles\n    - species:                                            # Species definition\n        verbose: false\n        fail_if_empty: true\n    - grid_post_processing                                # Grid memory compaction\n    - reduce_species_after_read                           # Update particle species\n    - init_rcut_max                                       # Update neighborhood distance and displacement tolerance\n    - print_domain                                        # Print Domain information\n    - performance_adviser: { verbose: true }              # Print performance advices\n    - do_init_temperature                                 # Initialize temperature if needed\n    - init_epilog                                         # Initialization epilogue\n    - species:                                            # Species definition recheck\n        verbose: true\n        fail_if_empty: true\n    - first_iteration                                     # Simulation first iteration\n    - compute_loop                                        # Simulation compute loop\n    - simulation_epilog                                   # Simulation finalization\n    - hw_device_finalize                                  # CUDA finalization\n</code></pre> <p>In this YAML block, some operators are mandatory. We provide in the next section a input deck example with the minimal information required. In addition, some of the YAML operators defined in the <code>simulation</code> block are defined in other configuration files located in <code>exaStamp/data/config</code>. However, the mandatory blocks to be defined by the user to build a minimal input deck for exaStamp are:</p> <pre><code>   - global         # Global control of simulation parameters\n   - species        # Definition of the particles' species\n   - compute_force  # Choice of the interatomic potential\n   - domain         # Definition of the simulation's domain\n   - input_data     # Population of the domain with particles\n</code></pre> <p>The next section provides a basic example on how to build an <code>exaStamp</code> input deck with the minimal information required.</p> <pre><code>graph TD\n  A[\"`**Initialization**\n  Detect GPU Support\n  Create Grid\n  Preinit cutoff radius`\"]\n\n  B[Creating Particle Regions]\n\n  C[\"`**Input Data**\n  lattice, xyz, MpiIO\n  + field initialization`\"]\n\n  D[\"`**Firs Timestep**\n  Init partcles\n  - move\n  - migration\n  - neighbor lists\n  - compute forces`\"]\n\n  E[Time Loop];\n  F[test];\n  G[coucou];\n\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt; D;\n  D --&gt; E;\n  E --&gt; F;\n  F --&gt; G\n  G---&gt;|\"`t inf tfin`\"| D;\n\n</code></pre> <p>B --&gt;|Yes| C[Hmm...];   C --&gt; D[Debug];   D --&gt; B;   B ----&gt;|No| E[Yay!];   E ===&gt; newLines[\"<code>Line1                     Line 2                     Line 3</code>\"]  </p>"},{"location":"microStamp/index.html","title":"microStamp mini application","text":"<p>A minimal version of exaStamp called microStamp is made available through the exaNBody platform. Only two interatomic potentials are available in microStamp namely the Lennard-Jones (LJ) potential and the Spectral Neighbor Analysis Potential (SNAP). microStamp allows for performing molecular dynamics simulation without the need to build and install the full exaStamp software. This has two advantages:</p> <ul> <li>A lightweight platform that allows to develop new tools and integrate them at the exaNBody level, thus enabling all applications built on exaNBody to benefit from it,</li> <li>Allow for a fast benchmark of two very different interatomic potentials where LJ is the fastest and simplest one while SNAP is quite computationally intensive and slower.</li> </ul> <p>The microStamp mini application main purpose is to allow for developers to deep dive into the exaNBody code for integrating new features.</p>"},{"location":"microStamp/cmake_installation.html","title":"Installation with CMake","text":"<p>At the <code>exaNBody</code> level, a mini MD app is avaiable for benchmark and optimization purposes. That mini app only contains the Lennard-Jones and the SNAP potentials. Below are the minimal instructions to build that mini app.</p> <p>The following installation consists in first building both the <code>ONIKA</code> HPC platform and the <code>exaNBody</code> particle simulation platform. Below are instructions for building both as well as final instruction for running the SNAP case where the initial system is read from a dump file.</p> <p>For all the codes, a single installation method through the use of <code>CMake</code> is provided, dedicated to both users and developer. The use of <code>CMake</code> allows the full support on both <code>CPU</code> and <code>GPU</code> architectures.</p>"},{"location":"microStamp/cmake_installation.html#minimal-requirements","title":"Minimal requirements","text":""},{"location":"microStamp/cmake_installation.html#yaml-library","title":"YAML library","text":"<p>All three platforms extensively use the <code>YAML</code> Library. To build <code>YAML</code> from sources, read the following instructions. Installations procedures using <code>spack</code>, <code>apt-get</code> or <code>CMake</code> are provided.</p> <p>Installation procedure for YAML</p> CMakeSpackapt-get install <pre><code># Retrieve YAML sources into temporary folder\nYAMLTMPFOLDER=${path_to_tmp_yaml}\nmkdir ${YAMLTMPFOLDER} &amp;&amp; cd ${YAMLTMPFOLDER}\ngit clone --depth 1 --branch yaml-cpp-0.6.3 git@github.com:jbeder/yaml-cpp.git\n\n# Define installation directory\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3\n\n# Build and install YAML from sources using CMake \ncd ${YAMLTMPFOLDER} &amp;&amp; mkdir build &amp;&amp; cd build\ncmake -DCMAKE_BUILD_TYPE=Debug \\\n      -DCMAKE_INSTALL_PREFIX=${YAML_CPP_INSTALL_DIR} \\\n      -DYAML_BUILD_SHARED_LIBS=OFF \\\n      -DYAML_CPP_BUILD_CONTRIB=ON \\\n      -DYAML_CPP_BUILD_TESTS=OFF \\\n      -DYAML_CPP_BUILD_TOOLS=OFF \\\n      -DYAML_CPP_INSTALL=ON \\\n      -DCMAKE_CXX_FLAGS=-fPIC \\\n      ../yaml-cpp\nmake -j4 install\nexport YAML_CPP_INSTALL_DIR=${YAML_CPP_INSTALL_DIR}/yaml-cpp-0.6.3\n# Remove temporary folder\ncd ../..\nrm -r ${YAMLTMPFOLDER}            \n</code></pre> <pre><code>spack install yaml-cpp@0.6.3\nspack load yaml-cpp@0.6.3\n</code></pre> <pre><code>sudo apt-get install libyaml-cpp-dev\n</code></pre> <p>At this point, you should have YAML installed on your system. Please note that the installation procedure of YAML from sources using <code>CMake</code> also works on HPC clusters. In the following, remember to add the `-DCMAKE_PREFIX_PATH=${YAML_CPP_INSTALL_DIR} argument to your cmake command.</p>"},{"location":"microStamp/cmake_installation.html#onika","title":"Onika","text":"<p><code>ONIKA</code> (Object Network Interface for Knit Applications), is a component based HPC software platform to build numerical simulation codes. It is the foundation for the <code>exaNBody</code> particle simulation platform but is not bound to N-Body problems nor other domain specific simulation code. Existing applications based on its building blocks include Molecular Dynamics, particle based fluid simulations using methods such as Smooth Particle Hydrodynamics (SPH) or rigid body simulations using methods such as Discrete Element Method (DEM). It uses industry grade standards and widely adopted technologies such as CMake and C++20 for development and build, <code>YAML</code> for user input files, MPI and OpenMP for parallel programming, Cuda and HIP for GPU acceleration. To build <code>ONIKA</code> from sources, read the following instructions.</p> <p>Installation procedure for ONIKA</p> UBUNTU CPUUBUNTU GPURhel x INTEL x CUDARhel x GCC x CUDA <pre><code># Adapt depending on where you want to download onika\ncd ${HOME}/dev\ngit clone git@github.com:Collab4exaNBody/onika.git\nONIKA_SRC_DIR=${HOME}/dev/onika\nONIKA_INSTALL_DIR=${HOME}/local/onika            \nmkdir build_onika &amp;&amp; cd build_onika\n\nONIKA_SETUP_ENV_COMMANDS=\"\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=OFF \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j4 install\n</code></pre> <pre><code># Adapt depending on where you want to download onika\ncd ${HOME}/dev\ngit clone git@github.com:Collab4exaNBody/onika.git\nONIKA_SRC_DIR=${HOME}/dev/onika\nONIKA_INSTALL_DIR=${HOME}/local/onika            \nmkdir build_onika &amp;&amp; cd build_onika\n\nONIKA_SETUP_ENV_COMMANDS=\"\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=ON \\\n      -DCMAKE_CUDA_COMPILER=${PATH_TO_NVCC} \\\n      -DCMAKE_CUDA_ARCHITECTURES=${ARCH} \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j4 install\n</code></pre> <pre><code>ONIKA_INSTALL_DIR=${HOME}/local/onika\nONIKA_SRC_DIR=${HOME}/dev/onika\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3/lib/cmake/yaml-cpp            \nONIKA_SETUP_ENV_COMMANDS=\"module purge ; module load gnu/11.2.0 nvhpc/24.3 inteloneapi/24.2.0 mpi/openmpi cmake/3.26.4\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\nCXX_COMPILER=`which icpx`\nC_COMPILER=`which icx`\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -DCMAKE_C_COMPILER=${C_COMPILER} \\\n      -DCMAKE_CXX_COMPILER=${CXX_COMPILER} \\\n      -DCMAKE_CXX_FLAGS=-diag-disable=15518,15552 \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=ON \\\n      -DCMAKE_CUDA_COMPILER=/ccc/products/cuda-12.4/system/default/bin/nvcc \\\n      -DCMAKE_CUDA_FLAGS=\"-ccbin ${CXX_COMPILER} -allow-unsupported-compiler\" \\\n      -DCMAKE_CUDA_ARCHITECTURES=80 \\\n      -DONIKA_MPIRUN_CMD=\"/usr/bin/ccc_mprun\" \\\n      -DMPIEXEC_EXECUTABLE=`which mpiexec` \\\n      -DMPIEXEC_MAX_NUMPROCS=32 \\\n      -DMPIEXEC_NUMCORE_FLAG=\"-c\" \\\n      -DMPIEXEC_NUMPROC_FLAG=\"-n\" \\\n      -DMPIEXEC_PREFLAGS=\"-pa100-bxi\" \\\n      -DMPIEXEC_PREFLAGS_DBG=\"-pa100-bxi;-Xall;xterm;-e\" \\\n      -DONIKA_ALWAYS_USE_MPIRUN=ON \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j32 install\n</code></pre> <pre><code>ONIKA_INSTALL_DIR=${HOME}/local/onika\nONIKA_SRC_DIR=${HOME}/dev/onika\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3/lib/cmake/yaml-cpp\nONIKA_SETUP_ENV_COMMANDS=\"module purge ; module load gnu/12.3.0 nvhpc/24.3 mpi/openmpi cmake/3.26.4\"\neval ${ONIKA_SETUP_ENV_COMMANDS}\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${ONIKA_INSTALL_DIR} \\\n      -Dyaml-cpp_DIR=${YAML_CPP_INSTALL_DIR} \\\n      -DONIKA_BUILD_CUDA=ON \\\n      -DCMAKE_CUDA_COMPILER=/ccc/products/cuda-12.4/system/default/bin/nvcc \\\n      -DCMAKE_CUDA_ARCHITECTURES=80 \\\n      -DONIKA_MPIRUN_CMD=\"/usr/bin/ccc_mprun\" \\\n      -DMPIEXEC_EXECUTABLE=`which mpiexec` \\\n      -DMPIEXEC_MAX_NUMPROCS=32 \\\n      -DMPIEXEC_NUMCORE_FLAG=\"-c\" \\\n      -DMPIEXEC_NUMPROC_FLAG=\"-n\" \\\n      -DMPIEXEC_PREFLAGS=\"-pa100-bxi\" \\\n      -DMPIEXEC_PREFLAGS_DBG=\"-pa100-bxi;-Xall;xterm;-e\" \\\n      -DONIKA_ALWAYS_USE_MPIRUN=ON \\\n      -DONIKA_SETUP_ENV_COMMANDS=\"${ONIKA_SETUP_ENV_COMMANDS}\" \\\n      ${ONIKA_SRC_DIR}\nmake -j32 install                \n</code></pre>"},{"location":"microStamp/cmake_installation.html#microstamp-in-exanbody","title":"microStamp in exaNBody","text":"<p><code>exaNBody</code> is a software platform to build-up numerical simulations solving N-Body like problems. Typical applications include Molecular Dynamics, particle based fluid simulations using methods such as Smooth Particle Hydrodynamics (SPH) or rigid body simulations using methods such as Discrete Element Method (DEM). It uses standard and widely adopted technologies such as C++20, YAML, OpenMP , Cuda or HIP. When installing <code>exaNBody</code>, first sourcing the <code>ONIKA</code> environment will automatically update whether CUDA is activated or not.</p> <p>Installation procedure for exaNBody</p> UBUNTURhel x INTELRhel x GCC <pre><code># Adapt depending on where you want to download ``exaNBody``\ncd ${HOME}/dev\ngit clone -b release-2.0 git@github.com:Collab4exaNBody/exaNBody.git\nXNB_SRC_DIR=${HOME}/dev/exaNBody\nXNB_INSTALL_DIR=${HOME}/local/exaNBody\nmkdir build_exaNBody &amp;&amp; cd build_exaNBody\n\nsource ${ONIKA_INSTALL_DIR}/bin/setup-env.sh\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XNB_INSTALL_DIR} \\\n      -Donika_DIR=${ONIKA_INSTALL_DIR} \\\n      -DEXANB_BUILD_CONTRIB_MD=ON \\\n      -DEXANB_BUILD_MICROSTAMP=ON \\\n      ${XNB_SRC_DIR}\nmake -j4 install\n</code></pre> <pre><code># Adapt depending on where you want to download ``exaNBody``\ncd ${HOME}/dev\ngit clone -b release-2.0 git@github.com:Collab4exaNBody/exaNBody.git\nXNB_SRC_DIR=${HOME}/dev/exaNBody\nXNB_INSTALL_DIR=${HOME}/local/exaNBody\nmkdir build_exaNBody &amp;&amp; cd build_exaNBody\n\nsource ${ONIKA_INSTALL_DIR}/bin/setup-env.sh\nCXX_COMPILER=`which icpx`\nC_COMPILER=`which icx`\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XNB_INSTALL_DIR} \\\n      -DCMAKE_C_COMPILER=${C_COMPILER} \\\n      -DCMAKE_CXX_COMPILER=${CXX_COMPILER} \\\n      -DCMAKE_CXX_FLAGS=-diag-disable=15518,15552 \\\n      -Donika_DIR=${ONIKA_INSTALL_DIR} \\\n      -DEXANB_BUILD_CONTRIB_MD=ON \\\n      -DEXANB_BUILD_MICROSTAMP=ON \\\n      ${XNB_SRC_DIR}    \nmake -j32 install\n</code></pre> <pre><code>XNB_INSTALL_DIR=${HOME}/local/exaNBody\nsource ${ONIKA_INSTALL_DIR}/bin/setup-env.sh\ncmake -DCMAKE_BUILD_TYPE=Release \\\n      -DCMAKE_INSTALL_PREFIX=${XNB_INSTALL_DIR} \\\n      -Donika_DIR=${ONIKA_INSTALL_DIR} \\\n      -DEXANB_BUILD_CONTRIB_MD=ON \\\n      -DEXANB_BUILD_MICROSTAMP=ON \\\n      ${XNB_SRC_DIR}\nmake -j32 install\n</code></pre>"},{"location":"microStamp/running.html","title":"Running microStamp","text":"<p>Basic YAML example for exaStamp</p> <pre><code>tar -zxvf benchmark-snap-new.tgz\ncd benchmark-snap-new/\n${ONIKA_INSTALL_DIR}/bin/onika-exec snap_from_dump.msp\n\n# To increase the number of cells (thus number of used cuda blocks)\n# and the total number of particles, one can add the --set-replicate_domain-repeat \"[Nx,Ny,Nz]\"\n# where Nx, Ny, Nz are the number of replication in each direction of the 3D space.\n\n# Example for replicating 8 times (2 in each direction):\n${ONIKA_INSTALL_DIR}/bin/onika-exec snap_from_dump.msp --set-replicate_domain-repeat \"[2,2,2]\"\n</code></pre> <p>Now that you have installed <code>onika</code>, <code>exaNBody</code> and <code>exaStamp</code>, you can create your simulation file using the <code>YAML</code> format. Please refer to the <code>Beginner's guide to exaStamp</code> section to learn how to build your first input deck! Once this file is constructed, you can run your simulation with a specified number of <code>MPI</code> processes and threads per <code>MPI</code> process. </p>"},{"location":"microStamp/running.html#simple-yaml-input-file","title":"Simple YAML input file","text":"<p>Below is an example of an <code>YAML</code> input file that allows to:</p> <ul> <li>Define a 3D-periodic simulation cell containing an FCC Copper sample</li> <li>Assign a gaussian random noise to particles positions and velocities</li> <li>Performs the time integration using a Velocity-Verlet scheme</li> <li>Uses a Langevin thermostat to maintain the temperature at 300. K</li> <li>Dumps <code>.xyz</code> files at a specific frequency for vizualization with <code>OVITO</code></li> </ul> <p>Basic YAML example for exaStamp</p> <pre><code># Choose the grid flavor\ngrid_flavor: grid_flavor_full\n\n# Define the species present in the system\nspecies:\n  - Cu: { mass: 63.546 Da , z: 29 , charge: 0.0 e- }\n\n# Interatomic potential\nsutton_chen_force:\n  rcut: 7.29 ang\n  parameters:\n    c: 3.317E+01\n    epsilon: 3.605E-21 J\n    a0: 0.327E-09 m\n    n: 9.050E+00\n    m: 5.005E+00\n\n# Force operator (interatomic potential + Langevin thermostat)\ncompute_force:\n  - sutton_chen_force\n  - langevin_thermostat: { T: 300. K, gamma: 0.1 ps^-1 }\n\n# System's creation\ninput_data:\n  - domain:\n      cell_size: 5.0 ang\n  - bulk_lattice:\n      structure: FCC\n      types: [ Cu, Cu, Cu, Cu]\n      size: [ 3.615 ang , 3.615 ang , 3.615 ang ]\n      repeat: [ 20, 20, 20 ]\n  - gaussian_noise_r:\n  - gaussian_noise_v:\n\n# Simulation parameters        \nglobal:\n  simulation_end_iteration: 1000\n  simulation_log_frequency: 20\n  simulation_dump_thermo_frequency: -1\n  simulation_dump_frequency: -1\n  rcut_inc: 1.0 ang\n  dt: 2.0e-3 ps\n  init_temperature: 5. K\n</code></pre>"},{"location":"microStamp/running.html#its-go-time","title":"It's go time","text":"<p>To run the above case, and depending if you installed <code>exaNBody</code> using <code>CMake</code> or <code>spack</code> follow these instructions:</p> <p>Running an microStamp simulation</p> CMakeSpack <pre><code>source ${XNB_BUILD_DIR}/exaNBody\nexport OMP_NUM_THREADS=10\nexport N_MPI=2\nmpirun -np ${N_MPI} ${XSP_BUILD_DIR}/exaNBody myinput.msp\n</code></pre> <pre><code>export OMP_NUM_THREADS=10\nexport N_MPI=2\nspack load exanbody\nmpirun -np ${N_NMPI} exaNBody myinput.msp\n</code></pre>"},{"location":"microStamp/spack_installation.html","title":"Installation with Spack","text":"<p>Installation with <code>Spack</code> is easy and preferable for users who don't want to develop in <code>microStamp</code> or <code>exaNBody</code>. Only stable versions are added when you install <code>exaNBody</code> with <code>Spack</code>, meaning that it doesn't provide you access to the development branches. In addition, the main branch of <code>exaNBody</code> will never be directly accessible via this installation method.</p>"},{"location":"microStamp/spack_installation.html#minimal-requirements","title":"Minimal requirements","text":""},{"location":"microStamp/spack_installation.html#spack-package-manager","title":"Spack package manager","text":"<p>Below are instructions to first retrieve spack sources and install it on your system. First, clone the corresponding git repository and source the appropriate environment.</p> <p>Clone Spack</p> <pre><code>cd ${HOME}/dev\ngit clone https://github.com/spack/spack.git\nexport SPACK_ROOT=${HOME}/dev/spack\nsource ${SPACK_ROOT}/share/spack/setup-env.sh\n</code></pre>"},{"location":"microStamp/spack_installation.html#yaml-library","title":"YAML library","text":"<p>All three platforms extensively use the <code>YAML</code> Library. To build <code>YAML</code> from sources, read the following instructions. Installations procedures using <code>spack</code>, <code>apt-get</code> or <code>CMake</code> are provided.</p> <p>Installation procedure for YAML</p> Spackapt-get installCMake <pre><code>spack install yaml-cpp@0.6.3\nspack load yaml-cpp@0.6.3\n</code></pre> <pre><code>sudo apt-get install libyaml-cpp-dev\n</code></pre> <pre><code># Retrieve YAML sources into temporary folder\nYAMLTMPFOLDER=${path_to_tmp_yaml}\nmkdir ${YAMLTMPFOLDER} &amp;&amp; cd ${YAMLTMPFOLDER}\ngit clone --depth 1 --branch yaml-cpp-0.6.3 git@github.com:jbeder/yaml-cpp.git\n\n# Define installation directory\nYAML_CPP_INSTALL_DIR=${HOME}/local/yaml-cpp-0.6.3\n\n# Build and install YAML from sources using CMake \ncd ${YAMLTMPFOLDER} &amp;&amp; mkdir build &amp;&amp; cd build\ncmake -DCMAKE_BUILD_TYPE=Debug \\\n      -DCMAKE_INSTALL_PREFIX=${YAML_CPP_INSTALL_DIR} \\\n      -DYAML_BUILD_SHARED_LIBS=OFF \\\n      -DYAML_CPP_BUILD_CONTRIB=ON \\\n      -DYAML_CPP_BUILD_TESTS=OFF \\\n      -DYAML_CPP_BUILD_TOOLS=OFF \\\n      -DYAML_CPP_INSTALL=ON \\\n      -DCMAKE_CXX_FLAGS=-fPIC \\\n      ../yaml-cpp\nmake -j4 install\nexport YAML_CPP_INSTALL_DIR=${YAML_CPP_INSTALL_DIR}/yaml-cpp-0.6.3\n# Remove temporary folder\ncd ../..\nrm -r ${YAMLTMPFOLDER}            \n</code></pre> <p>At this point, you should have YAML installed on your system. Please note that the installation procedure of YAML from sources using <code>CMake</code> also works on HPC clusters. In the following, remember to add the `-DCMAKE_PREFIX_PATH=${YAML_CPP_INSTALL_DIR} argument to your cmake command.</p>"},{"location":"microStamp/spack_installation.html#microstamp-in-exanbody","title":"microStamp in exaNBody","text":"<p>First, clone the <code>spack-repos</code> GitHub repository on your computer and add this repository to spack. This repository contains <code>onika</code> and <code>exaNBody</code> recipes that allow for their installation.</p> <p>Clone spack-repos</p> <pre><code>git clone https://github.com/Collab4exaNBody/spack-repos.git\nspack repo add spack-repos\n</code></pre> <p>Then, simply install <code>exaNBody</code>. To get access to the <code>microStamp</code> mini application, you need to require the variant <code>contribs</code> as follows:</p> <p>Install exaNBody</p> <pre><code>spack install exaNBody+contribs\n</code></pre> <p>If you have a GPU on your machine, you can also ask for a CUDA installation through the following command:</p> <p>Install exaStamp with CUDA support</p> <pre><code>spack install exastamp+contribs+cuda\n</code></pre> <p>The default version will be the latest stable release. However, you can also ask for a specific branch as follow:</p> <p>Finally, the commands listed above will install the appropriate version of <code>cmake</code>, <code>yaml-cpp</code>, <code>onika</code> and <code>exaNBody</code> and additional required packages by the <code>exaStamp</code> recipe. Eventually, to run an <code>exaStamp</code> case, do the following:</p> <p>Install exaStamp</p> <pre><code>spack load exanbdoy\nexaNBody myinput.msp\n</code></pre>"}]}